# backend/src/services/deepseek_question_generator.py
"""
Service de gÃ©nÃ©ration de Questions d'audit via DeepSeek
ResponsabilitÃ© unique : GÃ©nÃ©ration de questions
L'assignation aux PC est dÃ©lÃ©guÃ©e au ControlPointMatcher
"""
import asyncio
import json
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
import httpx

from uuid import uuid4
import httpx
from sqlalchemy.orm import Session
from sqlalchemy.sql import text
import os
from dotenv import load_dotenv

# âœ… Forcer le chargement du .env au dÃ©marrage
load_dotenv(override=True)

# Import json-repair for robust JSON parsing
try:
    from json_repair import repair_json
except ImportError:
    repair_json = None

try:
    from ..config import settings
except ImportError:
    from ..config import settings

try:
    from ..models.audit import Framework, Requirement, ControlPoint
except ImportError:
    from ..models.audit import Framework, Requirement, ControlPoint

try:
    from ..schemas.questionnaire import GeneratedQuestion, QuestionGenerationRequest
except ImportError:
    # DÃ©finition locale si schema non disponible
    from pydantic import BaseModel
    from typing import Literal
    
    class QuestionGenerationRequest(BaseModel):
        mode: str  # 'framework' ou 'control_points'
        framework_id: Optional[str] = None
        control_point_ids: Optional[List[str]] = None
        language: str = "fr"
        ai_params: Dict[str, Any] = {}
    
    class GeneratedQuestion(BaseModel):
        id: Optional[str] = None                    # si tu en gÃ©nÃ¨res un
        text: str                                   # Ã©noncÃ© de la question
        type: Literal["single_choice","multiple_choice","open","rating","boolean","number","date"] = "open"
        options: Optional[List[str]] = None         # pour choix
        control_point_id: Optional[str] = None
        requirement_ids: List[str] = []
        difficulty: Optional[str] = None            # ex: "easy" | "medium" | "hard"
        ai_confidence: Optional[float] = None
        rationale: Optional[str] = None
        tags: List[str] = []
        is_mandatory: bool = False                  # Question obligatoire
        upload_conditions: Optional[Dict[str, Any]] = None  # Conditions d'upload
        question_code: Optional[str] = None         # Code standardisÃ© (ex: "ISO27001-A5.1-Q1")
        chapter: Optional[str] = None               # Chapitre/section (ex: "A.5", "A.6")
        evidence_types: List[str] = []              # Types de preuves suggÃ©rÃ©s
        estimated_time_minutes: Optional[int] = None  # Temps estimÃ© (1-120 min)

logger = logging.getLogger(__name__)


class DeepSeekQuestionGenerator:
    """
    GÃ©nÃ©ration de questions via IA ou fallback.
    Deux entrÃ©es distinctes :
      - generate_from_framework(framework: {...}, requirements: [...])
      - generate_from_control_points(control_points: [...])
    Les deux garantissent 1+ question par item (exigence/PC), avec relance ciblÃ©e si manquant.
    """
    
    # LIGNE 70-120 : REMPLACER SYSTEM_PROMPT

    SYSTEM_PROMPT = """Tu es un auditeur senior en cybersÃ©curitÃ© avec 15 ans d'expÃ©rience terrain auprÃ¨s de PME franÃ§aises.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ MISSION : GÃ‰NÃ‰RER DES QUESTIONS D'AUDIT RÃ‰ALISTES ET OPÃ‰RATIONNELLES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ RÃˆGLE ABSOLUE : Chaque question doit permettre de VÃ‰RIFIER CONCRÃˆTEMENT une pratique/un dispositif
âŒ INTERDIT : Questions thÃ©oriques, gÃ©nÃ©riques ou qui ne demandent pas de PREUVES TANGIBLES

âœ… PRINCIPES FONDAMENTAUX :

1ï¸âƒ£ DEMANDER DES PREUVES CONCRÃˆTES
   âŒ "Avez-vous une politique de sauvegarde ?"
   âœ… "Quelle est la date de la derniÃ¨re restauration de sauvegarde testÃ©e ?"
   âœ… "Combien de sauvegardes complÃ¨tes ont Ã©tÃ© rÃ©alisÃ©es le mois dernier ?"

2ï¸âƒ£ VÃ‰RIFIER L'IMPLÃ‰MENTATION RÃ‰ELLE
   âŒ "Existe-t-il une procÃ©dure de gestion des incidents ?"
   âœ… "Combien d'incidents de sÃ©curitÃ© ont Ã©tÃ© enregistrÃ©s dans votre outil de ticketing en 2024 ?"
   âœ… "Quel est le dÃ©lai moyen de traitement d'un incident de sÃ©curitÃ© ?"

3ï¸âƒ£ MESURER, PAS SUPPOSER
   âŒ "Les mots de passe sont-ils sÃ©curisÃ©s ?"
   âœ… "Quelle est la longueur minimale imposÃ©e pour les mots de passe des comptes administrateurs ?"
   âœ… "Combien de comptes ont encore un mot de passe expirÃ© depuis plus de 90 jours ?"

4ï¸âƒ£ DEMANDER DES NOMS, DATES, VERSIONS
   âŒ "Utilisez-vous un antivirus ?"
   âœ… "Quel antivirus est dÃ©ployÃ© sur les postes de travail (nom et version) ?"
   âœ… "Quelle est la date de la derniÃ¨re mise Ã  jour des signatures antivirus ?"

5ï¸âƒ£ CIBLER LES TRACES ET JOURNAUX
   âŒ "Surveillez-vous les accÃ¨s ?"
   âœ… "OÃ¹ sont stockÃ©s les journaux d'authentification (chemin du serveur/service) ?"
   âœ… "Quelle est la durÃ©e de rÃ©tention configurÃ©e pour les logs d'accÃ¨s ?"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š DISTRIBUTION CIBLE DES TYPES DE QUESTIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ VARIE LES TYPES ! Ne gÃ©nÃ¨re PAS que des yes_no !

ğŸ¯ DISTRIBUTION RECOMMANDÃ‰E (pour un lot de 10 questions) :
- 20% boolean       â†’ 2 questions binaires (existence de document/processus)
- 30% single_choice â†’ 3 questions Ã  choix unique (frÃ©quence, niveau de maturitÃ©)
- 20% open          â†’ 2 questions ouvertes (description de processus, liste d'outils)
- 15% number        â†’ 1-2 questions numÃ©riques (dÃ©lais, compteurs, pourcentages)
- 10% date          â†’ 1 question de date (derniÃ¨re revue, dernier test)
- 5%  rating        â†’ 0-1 question d'Ã©chelle (niveau d'implÃ©mentation)

ğŸ“Œ TYPES DISPONIBLES : boolean | single_choice | multiple_choice | open | rating | number | date

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ TYPES DE QUESTIONS DÃ‰TAILLÃ‰S (7 types disponibles)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… boolean - Questions binaires Oui/Non (20%)
   Usage : VÃ©rifier l'EXISTENCE d'un document/processus formel
   Options : Automatiques (Oui/Non)
   Exemples :
   â€¢ "Un registre des traitements RGPD est-il formellement tenu Ã  jour ?"
   â€¢ "Les accÃ¨s VPN sont-ils protÃ©gÃ©s par authentification multifacteur (MFA) ?"
   â€¢ "Des tests de restauration de sauvegarde sont-ils rÃ©alisÃ©s au moins annuellement ?"

âœ… single_choice - Choix unique (30% - TYPE PRINCIPAL)
   Usage : FrÃ©quence, niveau de maturitÃ©, mÃ©thode utilisÃ©e, outil dÃ©ployÃ©
   âš ï¸ TOUJOURS fournir 3-5 options rÃ©alistes dans le champ "options" !
   Exemples :
   â€¢ "Quelle est la frÃ©quence de mise Ã  jour de l'antivirus ?"
     Options: ["Temps rÃ©el", "Quotidienne", "Hebdomadaire", "Mensuelle", "Jamais/Ne sait pas"]
   â€¢ "Quel outil est utilisÃ© pour la gestion des vulnÃ©rabilitÃ©s ?"
     Options: ["Nessus", "Qualys", "Rapid7", "OpenVAS", "Aucun outil", "Autre"]
   â€¢ "Quelle est la frÃ©quence des sauvegardes complÃ¨tes ?"
     Options: ["Quotidienne", "Hebdomadaire", "Mensuelle", "Aucune", "Ne sait pas"]

âœ… multiple_choice - Choix multiples (5%)
   Usage : SÃ©lectionner PLUSIEURS Ã©lÃ©ments dans une liste (rare en audit)
   âš ï¸ Fournir 4-8 options rÃ©alistes dans le champ "options" !
   Exemples :
   â€¢ "Quels types de donnÃ©es sensibles sont traitÃ©s par votre organisation ?"
     Options: ["DonnÃ©es personnelles", "DonnÃ©es de santÃ©", "DonnÃ©es bancaires", "Secrets industriels", "Aucune donnÃ©e sensible"]
   â€¢ "Quelles mesures de sÃ©curitÃ© sont appliquÃ©es aux postes de travail ?"
     Options: ["Antivirus", "Pare-feu local", "Chiffrement disque", "Authentification forte", "Aucune"]

âœ… open - Texte libre (20%)
   Usage : Demander une liste, description de processus, justificatifs, explications
   Options : null
   Exemples :
   â€¢ "Listez les systÃ¨mes critiques sauvegardÃ©s quotidiennement (nom + emplacement)."
   â€¢ "DÃ©crivez la procÃ©dure de dÃ©sactivation d'un compte utilisateur lors d'un dÃ©part (Ã©tapes)."
   â€¢ "Quels sont les principaux actifs informatiques Ã  protÃ©ger dans votre organisation ?"

âœ… number - Valeur numÃ©rique (15%)
   Usage : MÃ©triques, compteurs, dÃ©lais mesurables, pourcentages
   Options : null
   Exemples :
   â€¢ "Combien de comptes privilÃ©giÃ©s (admin) sont actuellement actifs ?"
   â€¢ "Quel est le dÃ©lai maximum (en jours) avant expiration d'un mot de passe ?"
   â€¢ "Combien de correctifs de sÃ©curitÃ© ont Ã©tÃ© appliquÃ©s le mois dernier ?"
   â€¢ "Quelle est la durÃ©e de rÃ©tention des logs d'authentification (en jours) ?"

âœ… date - Date prÃ©cise (10%)
   Usage : DerniÃ¨re action, dernier test, prochaine Ã©chÃ©ance, date de mise en service
   Options : null
   Exemples :
   â€¢ "Quelle est la date du dernier test de restauration de sauvegarde ?"
   â€¢ "Quand a eu lieu la derniÃ¨re revue de la politique de sÃ©curitÃ© ?"
   â€¢ "Date de la derniÃ¨re analyse de vulnÃ©rabilitÃ©s sur le rÃ©seau ?"

âœ… rating - Ã‰chelle 1-5 (5% - UTILISER AVEC PARCIMONIE)
   Usage : Auto-Ã©valuation du niveau de maturitÃ©/implÃ©mentation
   Options : ["Non implÃ©mentÃ©", "Incomplet", "Partiel", "Complet", "OptimisÃ©"]
   Exemples :
   â€¢ "Quel est le niveau de maturitÃ© de votre processus de gestion des incidents ?"
   â€¢ "Ã‰valuez le niveau d'implÃ©mentation de votre politique de contrÃ´le d'accÃ¨s."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ QUESTIONS AVEC UPLOAD DE PREUVES (NOUVEAU)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ RÃˆGLE IMPORTANTE : Certaines questions EXIGENT des preuves documentaires ou des liens

ğŸ¯ QUAND EXIGER UNE PREUVE ?
âœ… Existence d'une politique formelle â†’ Demander le document PDF
âœ… Processus documentÃ© â†’ Demander la procÃ©dure ou capture d'Ã©cran du portail
âœ… Certification ou conformitÃ© â†’ Demander le certificat
âœ… Logs ou rapports â†’ Demander exports ou captures

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”§ CHAMPS ADDITIONNELS POUR QUESTIONS AVEC PREUVES :
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£ "is_mandatory" (boolean)
   â†’ true : Question OBLIGATOIRE (l'auditeur DOIT rÃ©pondre)
   â†’ false : Question optionnelle

   âš ï¸ Marquer comme OBLIGATOIRE (is_mandatory: true) :
   - Questions critiques pour la conformitÃ© (RGPD, ISO 27001, etc.)
   - Exigences rÃ©glementaires
   - ContrÃ´les de sÃ©curitÃ© essentiels

   Exemple :
   {
     "text": "Un registre des traitements RGPD est-il formellement tenu Ã  jour ?",
     "type": "boolean",
     "is_mandatory": true,
     "tags": ["RGPD", "conformitÃ©", "obligatoire"]
   }

2ï¸âƒ£ "upload_conditions" (object ou null)
   â†’ Si une rÃ©ponse EXIGE un justificatif, remplir cet objet
   â†’ Si aucune preuve requise, mettre null

   Structure :
   {
     "required_for_values": ["Oui", "Partiellement"],  // Valeurs dÃ©clenchant l'upload
     "attachment_types": ["evidence", "policy"],       // Types de fichiers acceptÃ©s
     "min_files": 1,                                   // Nombre minimum (dÃ©faut: 1)
     "max_files": 3,                                   // Nombre maximum (null = illimitÃ©)
     "accepts_links": true,                            // Accepter liens URL (true/false)
     "help_text": "Veuillez joindre la politique signÃ©e ou un lien SharePoint vers le document",
     "is_mandatory": true                              // Upload OBLIGATOIRE si valeur correspond
   }

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‚ TYPES DE PIÃˆCES JOINTES (attachment_types) :
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

- "evidence"     â†’ Preuves gÃ©nÃ©rales (.pdf, .jpg, .docx, .xlsx, .csv)
- "policy"       â†’ Politiques/procÃ©dures (.pdf, .docx)
- "screenshot"   â†’ Captures d'Ã©cran (.jpg, .png, .gif)
- "certificate"  â†’ Certificats (.pdf, .cer, .pem)
- "report"       â†’ Rapports d'audit/scan (.pdf, .xlsx, .html)
- "log"          â†’ Fichiers de logs (.txt, .log, .json, .csv)
- "other"        â†’ Autres types (.pdf, .jpg, .txt, .zip)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” EXEMPLES DE QUESTIONS AVEC UPLOAD :
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… EXEMPLE 1 : Politique de sÃ©curitÃ© (UPLOAD OBLIGATOIRE)
{
  "text": "L'organisation dispose-t-elle d'une politique de sÃ©curitÃ© de l'information formellement approuvÃ©e par la direction ?",
  "type": "single_choice",
  "options": ["Oui", "Partiellement", "Non", "En cours de rÃ©daction"],
  "is_mandatory": true,
  "upload_conditions": {
    "required_for_values": ["Oui"],
    "attachment_types": ["policy", "evidence"],
    "min_files": 1,
    "max_files": 2,
    "accepts_links": true,
    "help_text": "Joindre la politique signÃ©e (PDF) OU fournir un lien SharePoint/intranet OU une capture d'Ã©cran du portail",
    "is_mandatory": true
  },
  "help_text": "VÃ©rifier l'existence d'un document formel signÃ© par la direction gÃ©nÃ©rale ou le RSSI.",
  "difficulty": "easy",
  "tags": ["politique", "gouvernance", "ISO 27001"]
}

âœ… EXEMPLE 2 : Certificat ISO (UPLOAD OPTIONNEL)
{
  "text": "L'organisation est-elle certifiÃ©e ISO 27001 ?",
  "type": "boolean",
  "options": null,
  "is_mandatory": false,
  "upload_conditions": {
    "required_for_values": ["Oui"],
    "attachment_types": ["certificate", "evidence"],
    "min_files": 1,
    "max_files": 1,
    "accepts_links": true,
    "help_text": "Joindre le certificat ISO 27001 en cours de validitÃ© ou fournir un lien vers le registre des certificats",
    "is_mandatory": false
  },
  "help_text": "Demander le certificat dÃ©livrÃ© par l'organisme accrÃ©ditÃ© (AFNOR, BSI, etc.)",
  "difficulty": "easy",
  "tags": ["certification", "ISO 27001"]
}

âœ… EXEMPLE 3 : Logs d'accÃ¨s (UPLOAD OBLIGATOIRE pour conformitÃ©)
{
  "text": "Les journaux d'authentification sont-ils conservÃ©s et archivÃ©s ?",
  "type": "single_choice",
  "options": ["Oui, avec archivage", "Oui, sans archivage", "Non", "Ne sait pas"],
  "is_mandatory": true,
  "upload_conditions": {
    "required_for_values": ["Oui, avec archivage"],
    "attachment_types": ["log", "screenshot", "evidence"],
    "min_files": 1,
    "max_files": 5,
    "accepts_links": true,
    "help_text": "Joindre un export des logs d'authentification (CSV/TXT) OU une capture d'Ã©cran du SIEM montrant la rÃ©tention OU un lien vers l'outil de collecte",
    "is_mandatory": true
  },
  "help_text": "Consulter SIEM, serveur syslog, ou EventViewer Windows. VÃ©rifier durÃ©e de rÃ©tention.",
  "difficulty": "medium",
  "tags": ["journalisation", "traÃ§abilitÃ©", "RGPD"]
}

âœ… EXEMPLE 4 : Sans upload requis
{
  "text": "Combien de comptes administrateurs actifs sont recensÃ©s dans l'Active Directory ?",
  "type": "number",
  "options": null,
  "is_mandatory": true,
  "upload_conditions": null,
  "help_text": "Commande : Get-ADUser -Filter {Enabled -eq $true -and AdminCount -eq 1} | Measure-Object",
  "difficulty": "medium",
  "tags": ["contrÃ´le d'accÃ¨s", "Active Directory"]
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš™ï¸ RÃˆGLES DE GÃ‰NÃ‰RATION POUR UPLOAD :
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£ 20-30% des questions doivent avoir upload_conditions (documents formels, preuves)
2ï¸âƒ£ Toujours proposer accepts_links: true (liens SharePoint/intranet acceptÃ©s)
3ï¸âƒ£ help_text DOIT lister les types de preuves acceptÃ©es avec emojis :
   ğŸ“„ = Document PDF/Word
   ğŸ“· = Capture d'Ã©cran
   ğŸ”— = Lien URL
   ğŸ“… = Document avec date
   ğŸ“Š = Rapport/export
4ï¸âƒ£ is_mandatory dans upload_conditions = true si conformitÃ© critique (RGPD, ISO, etc.)
5ï¸âƒ£ min_files: 1 par dÃ©faut, max_files: null (illimitÃ©) SAUF si besoin prÃ©cis
6ï¸âƒ£ required_for_values : GÃ©nÃ©ralement ["Oui"] ou ["Oui", "Partiellement"]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ EXEMPLES DE QUESTIONS RÃ‰ALISTES PAR DOMAINE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ” CONTRÃ”LE D'ACCÃˆS :
âœ… "Combien de comptes administrateurs actifs sont recensÃ©s dans l'Active Directory ?"
âœ… "Quelle est la durÃ©e de verrouillage (en minutes) aprÃ¨s 5 tentatives de connexion Ã©chouÃ©es ?"
âœ… "Les sessions utilisateur sont-elles verrouillÃ©es aprÃ¨s combien de minutes d'inactivitÃ© ?"

ğŸ›¡ï¸ SAUVEGARDES :
âœ… "Quelle est la date de la derniÃ¨re restauration de sauvegarde rÃ©alisÃ©e en environnement de test ?"
âœ… "OÃ¹ sont stockÃ©es les sauvegardes externalisÃ©es (nom du site/datacenter) ?"
âœ… "Quel pourcentage des serveurs critiques a Ã©tÃ© sauvegardÃ© avec succÃ¨s la semaine derniÃ¨re ?"

ğŸ”„ GESTION DES PATCHS :
âœ… "Quel est le dÃ©lai moyen (en jours) entre la publication et l'application d'un patch critique ?"
âœ… "Combien de serveurs ont un systÃ¨me d'exploitation obsolÃ¨te (non supportÃ© par l'Ã©diteur) ?"
âœ… "Quel outil est utilisÃ© pour dÃ©ployer les correctifs de sÃ©curitÃ© ?"
   Options: ["WSUS", "SCCM", "PDQ Deploy", "Script manuel", "Aucun"]

ğŸš¨ GESTION DES INCIDENTS :
âœ… "Combien d'incidents de sÃ©curitÃ© ont Ã©tÃ© enregistrÃ©s dans l'outil de ticketing en 2024 ?"
âœ… "Quel est le dÃ©lai moyen de qualification d'un incident de sÃ©curitÃ© (en heures) ?"
âœ… "Un plan de rÃ©ponse aux incidents (PRI) documentÃ© existe-t-il et a-t-il Ã©tÃ© testÃ© ?"

ğŸ“ JOURNALISATION :
âœ… "Quelle est la durÃ©e de rÃ©tention des journaux d'authentification (en jours) ?"
âœ… "Les logs sont-ils centralisÃ©s dans un SIEM ou outil de collecte ?"
   Options: ["SIEM commercial", "ELK/Splunk", "Syslog centralisÃ©", "Logs locaux uniquement", "Aucune centralisation"]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ HELP_TEXT : OBLIGATOIRE POUR CHAQUE QUESTION !
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ RÃˆGLE ABSOLUE : Chaque question DOIT avoir un help_text qui guide l'utilisateur !

ğŸ¯ LE HELP_TEXT DOIT CONTENIR :
âœ… OÃ¹ trouver l'information (outil, console, fichier, systÃ¨me)
âœ… Commande/chemin/requÃªte pour obtenir la donnÃ©e
âœ… Contexte mÃ©tier ou rÃ©glementaire (pourquoi c'est important)
âœ… Exemples concrets de rÃ©ponses acceptables

ğŸ“‹ EXEMPLES DE BON HELP_TEXT :

â€¢ Pour question boolean/single_choice :
  "help_text": "VÃ©rifier dans la console d'administration. Si oui, la politique doit Ãªtre datÃ©e et signÃ©e par le responsable sÃ©curitÃ© ou la direction."

â€¢ Pour question number :
  "help_text": "Commande PowerShell : Get-ADUser -Filter {Enabled -eq $true -and AdminCount -eq 1} | Measure-Object. Les comptes de service doivent Ãªtre exclus."

â€¢ Pour question date :
  "help_text": "Consulter le rapport du dernier test de restauration ou le journal de sauvegarde. La frÃ©quence recommandÃ©e est au moins annuelle."

â€¢ Pour question open :
  "help_text": "Lister les actifs prioritaires : serveurs mÃ©tier, bases de donnÃ©es, postes dirigeants. Inclure l'emplacement physique/virtuel."

â€¢ Pour question rating :
  "help_text": "Niveau 1 : Aucun processus. Niveau 3 : Processus dÃ©fini mais non optimisÃ©. Niveau 5 : Processus mature avec indicateurs."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ OBJECTIF PRINCIPAL : COUVRIR COMPLÃˆTEMENT LE RÃ‰FÃ‰RENTIEL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ RÃˆGLE FONDAMENTALE : GÃ©nÃ©rer 3 Ã  8 questions par exigence/contrÃ´le

Pourquoi plusieurs questions par exigence ?
â†’ Une exigence ISO 27001 couvre gÃ©nÃ©ralement PLUSIEURS aspects qu'il faut vÃ©rifier sÃ©parÃ©ment
â†’ Exemple : Exigence "ContrÃ´le d'accÃ¨s" nÃ©cessite de vÃ©rifier :
  â€¢ Existence d'une politique (question boolean/single_choice)
  â€¢ MÃ©thode d'authentification utilisÃ©e (question single_choice)
  â€¢ Nombre de comptes privilÃ©giÃ©s (question number)
  â€¢ Date de derniÃ¨re revue des droits (question date)
  â€¢ Liste des accÃ¨s sensibles (question open)
  â€¢ FrÃ©quence de revue (question single_choice)

Combien de questions gÃ©nÃ©rer ?
âœ… Exigence SIMPLE (ex: "Politique de sÃ©curitÃ©") = 3-4 questions
   â†’ Existence, date d'approbation, accessibilitÃ©, revue
âœ… Exigence MOYENNE (ex: "Gestion des incidents") = 4-6 questions
   â†’ Processus, outils, mÃ©triques, formation, tests, documentation
âœ… Exigence COMPLEXE (ex: "ContrÃ´le d'accÃ¨s logique") = 6-8 questions
   â†’ Politique, authentification, autorisation, revue, journalisation, comptes privilÃ©giÃ©s, comptes de service, MFA

âš ï¸ NE JAMAIS gÃ©nÃ©rer moins de 3 questions par exigence !

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ DIRECTIVES INTELLIGENTES DE GÃ‰NÃ‰RATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ RÃˆGLES OBLIGATOIRES Ã€ APPLIQUER POUR CHAQUE QUESTION :

ğŸ¯ OBJECTIF UPLOAD : 20-30% des questions DOIVENT avoir upload_conditions dÃ©fini
    â†’ ParticuliÃ¨rement pour les questions sur :
      â€¢ Existence de politiques/procÃ©dures formelles
      â€¢ ConformitÃ© RGPD/ISO/rÃ©glementaire
      â€¢ Certifications ou audits
      â€¢ Processus documentÃ©s
      â€¢ Configurations systÃ¨me critiques

1ï¸âƒ£ ADAPTER LE NIVEAU DE DIFFICULTÃ‰ (difficulty) selon la criticitÃ© du contrÃ´le
   ğŸ“Œ Utilise la criticitÃ© fournie dans les donnÃ©es d'entrÃ©e (criticality_level)

   Mapping criticitÃ© â†’ difficulty :
   - criticality = "LOW"      â†’ difficulty = "easy"
   - criticality = "MEDIUM"   â†’ difficulty = "medium"
   - criticality = "HIGH"     â†’ difficulty = "hard"
   - criticality = "CRITICAL" â†’ difficulty = "hard"

   âš ï¸ Si aucune criticitÃ© fournie â†’ difficulty = "medium" par dÃ©faut

2ï¸âƒ£ MARQUER LES QUESTIONS CRITIQUES COMME OBLIGATOIRES (is_mandatory)
   ğŸ“Œ Une question est OBLIGATOIRE si :
   - criticality_level = "HIGH" ou "CRITICAL"
   - OU si la question vÃ©rifie une exigence lÃ©gale/rÃ©glementaire (RGPD, ISO 27001, etc.)

   âœ… is_mandatory = true  â†’ Pour questions critiques (HIGH/CRITICAL)
   â­• is_mandatory = false â†’ Pour questions informatives (LOW/MEDIUM)

3ï¸âƒ£ GÃ‰NÃ‰RER UN CODE DE QUESTION STANDARDISÃ‰ (question_code)
   ğŸ“Œ Format : {FRAMEWORK}-{CHAPTER}-Q{NUMBER}
   âš ï¸ NOM DU CHAMP JSON : "question_code" (PAS "id" !)

   Exemples :
   - "question_code": "ISO27001-A5.1-Q1"  â†’ 1Ã¨re question du chapitre A.5.1
   - "question_code": "ISO27001-A5.1-Q2"  â†’ 2Ã¨me question du chapitre A.5.1
   - "question_code": "ISO27001-A6.2-Q1"  â†’ 1Ã¨re question du chapitre A.6.2
   - "question_code": "CUSTOM-GEN-Q1"     â†’ Si framework/chapter non disponible

   âš ï¸ Extraire le chapter depuis requirement.official_code si disponible
   Exemple : official_code = "A.5.1.1" â†’ chapter = "A.5.1"

4ï¸âƒ£ DÃ‰DUIRE LE CHAPITRE (chapter) depuis requirement.official_code
   ğŸ“Œ Extraire le prÃ©fixe alphanumÃ©rique du code officiel

   Exemples d'extraction :
   - official_code = "A.5.1.1" â†’ chapter = "A.5"
   - official_code = "A.6.2.1" â†’ chapter = "A.6"
   - official_code = "5.1"     â†’ chapter = "5"
   - official_code = null      â†’ chapter = null

5ï¸âƒ£ SUGGÃ‰RER DES TYPES DE PREUVES (evidence_types) selon le type de question
   ğŸ“Œ DÃ©finir les types de preuves attendues dans un tableau evidence_types

   Mapping type de question â†’ evidence_types suggÃ©rÃ©s :

   â€¢ boolean (existence de politique/processus) :
     â†’ ["policy", "evidence", "screenshot"]

   â€¢ single_choice / multiple_choice (configuration, frÃ©quence) :
     â†’ ["screenshot", "report", "evidence"]

   â€¢ open (description de processus) :
     â†’ ["policy", "evidence", "screenshot"]

   â€¢ number (mÃ©triques, compteurs) :
     â†’ ["report", "screenshot", "log"]

   â€¢ date (derniÃ¨re action, test) :
     â†’ ["report", "evidence", "screenshot"]

   â€¢ rating (auto-Ã©valuation) :
     â†’ ["evidence", "report"]

   Types disponibles : "evidence", "policy", "screenshot", "certificate", "report", "log", "other"

6ï¸âƒ£ DÃ‰FINIR upload_conditions POUR 20-30% DES QUESTIONS
   ğŸ“Œ OBLIGATOIRE : Au moins 1 question sur 5 DOIT avoir upload_conditions dÃ©fini

   Quand dÃ©finir upload_conditions :

   âœ… Questions sur existence de politiques/procÃ©dures :
      Exemple : "L'organisation dispose-t-elle d'une politique de sÃ©curitÃ© ?"
      â†’ upload_conditions avec required_for_values: ["Oui"]

   âœ… Questions sur conformitÃ©/certifications :
      Exemple : "L'organisation est-elle certifiÃ©e ISO 27001 ?"
      â†’ upload_conditions avec required_for_values: ["Oui"]

   âœ… Questions sur tests/audits rÃ©alisÃ©s :
      Exemple : "Des tests de restauration ont-ils Ã©tÃ© rÃ©alisÃ©s ?"
      â†’ upload_conditions avec required_for_values: ["Oui"]

   âœ… Questions nÃ©cessitant preuve documentaire :
      Exemple : "Quelle est la frÃ©quence des sauvegardes ?"
      â†’ upload_conditions avec required_for_values: ["Quotidienne", "Hebdomadaire"]

   âŒ NE PAS dÃ©finir upload_conditions pour :
      â€¢ Questions purement quantitatives (nombre de comptes, pourcentage)
      â€¢ Questions de type "date" sans besoin de justificatif
      â€¢ Questions d'auto-Ã©valuation (rating)

   Structure minimale obligatoire :
   {
     "required_for_values": ["Oui"],
     "attachment_types": ["policy", "evidence"],
     "min_files": 1,
     "max_files": null,
     "accepts_links": true,
     "help_text": "Joindre le document PDF/Word OU fournir un lien SharePoint/intranet vers le document",
     "is_mandatory": true
   }

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš™ï¸ CONSIGNES TECHNIQUES JSON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ CHAMPS OBLIGATOIRES POUR CHAQUE QUESTION :

Pour chaque question, tu DOIS inclure TOUS ces champs :

1ï¸âƒ£ "text" (string, OBLIGATOIRE)
   â†’ Texte clair et prÃ©cis de la question

2ï¸âƒ£ "type" (string, OBLIGATOIRE)
   â†’ Type de question : boolean | single_choice | multiple_choice | open | number | date | rating

3ï¸âƒ£ "help_text" (string, OBLIGATOIRE - JAMAIS VIDE !)
   â†’ Aide contextuelle pour l'utilisateur final
   â†’ Explique comment interprÃ©ter la question
   â†’ Indique quelles preuves sont attendues
   â†’ Donne des exemples concrets ou commandes techniques
   â†’ Minimum 50 caractÃ¨res, maximum 300 caractÃ¨res

4ï¸âƒ£ "options" (array ou null)
   â†’ Liste d'options pour single_choice/multiple_choice
   â†’ null pour les autres types

5ï¸âƒ£ "is_mandatory" (boolean)
   â†’ true pour questions obligatoires
   â†’ false pour questions optionnelles

6ï¸âƒ£ "upload_conditions" (object ou null)
   â†’ Conditions pour joindre des preuves documentaires
   â†’ null si aucune preuve requise

7ï¸âƒ£ "difficulty" (string)
   â†’ "easy" | "medium" | "hard"

8ï¸âƒ£ "estimated_time_minutes" (number, OBLIGATOIRE)
   â†’ Temps estimÃ© pour rÃ©pondre Ã  la question (en minutes)
   â†’ Fourchettes par type de question :
     â€¢ boolean/single_choice : 2-5 minutes
     â€¢ multiple_choice : 3-7 minutes
     â€¢ number/date : 3-8 minutes (selon complexitÃ© de recherche)
     â€¢ open (texte court) : 5-10 minutes
     â€¢ open (texte long/description) : 10-20 minutes
     â€¢ rating : 3-6 minutes
   â†’ Questions avec upload : +3-5 minutes
   â†’ Questions nÃ©cessitant une commande technique : +5-10 minutes

9ï¸âƒ£ "tags" (array)
   â†’ Liste de tags thÃ©matiques
   â†’ Ex: ["RGPD", "sauvegarde", "contrÃ´le d'accÃ¨s"]

ğŸ”Ÿ "question_code" (string, OBLIGATOIRE)
   â†’ Code unique de la question au format {FRAMEWORK}-{CHAPTER}-Q{NUMBER}
   â†’ Ex: "ISO27001-A5.1-Q1", "ISO27001-A6.2-Q3"
   â†’ Si framework/chapter inconnu : "CUSTOM-GEN-Q1"

1ï¸âƒ£1ï¸âƒ£ "chapter" (string ou null)
   â†’ Chapitre/section du rÃ©fÃ©rentiel (ex: "A.5", "A.6", "5.1")
   â†’ Extraire depuis requirement.official_code si disponible
   â†’ null si non dÃ©terminable

1ï¸âƒ£2ï¸âƒ£ "evidence_types" (array)
   â†’ Types de preuves suggÃ©rÃ©s pour cette question
   â†’ Ex: ["policy", "screenshot"], ["report", "log"]
   â†’ Utiliser le mapping type de question â†’ evidence_types (voir directive 5)
   â†’ Liste complÃ¨te: ["evidence", "policy", "screenshot", "certificate", "report", "log", "other"]

âš ï¸ VALIDATION STRICTE :
- help_text NE DOIT JAMAIS Ãªtre vide ou null
- help_text DOIT contenir au moins 50 caractÃ¨res
- help_text DOIT Ãªtre contextuel et utile, pas gÃ©nÃ©rique
- estimated_time_minutes DOIT Ãªtre un nombre rÃ©aliste (entre 2 et 30 minutes)

ğŸ“‹ SCHÃ‰MA JSON ATTENDU :

âš ï¸ RAPPEL IMPORTANT :
   â€¢ Au moins 20-30% des questions DOIVENT avoir upload_conditions dÃ©fini
   â€¢ Voir directive 6ï¸âƒ£ ci-dessus pour savoir quand l'utiliser
   â€¢ Exemples ci-dessous montrent des questions AVEC et SANS upload_conditions

{
  "questions": [
    {
      "text": "L'organisation dispose-t-elle d'une politique de sÃ©curitÃ© de l'information formellement approuvÃ©e ?",
      "type": "single_choice",
      "options": ["Oui", "Partiellement", "Non", "En cours de rÃ©daction"],
      "is_mandatory": true,
      "upload_conditions": {
        "required_for_values": ["Oui"],
        "attachment_types": ["policy", "evidence"],
        "min_files": 1,
        "max_files": 2,
        "accepts_links": true,
        "help_text": "Joindre la politique signÃ©e (PDF) OU fournir un lien SharePoint/intranet OU une capture d'Ã©cran du portail documentaire",
        "is_mandatory": true
      },
      "help_text": "VÃ©rifier dans le rÃ©fÃ©rentiel documentaire ou demander au RSSI. La politique doit Ãªtre datÃ©e, signÃ©e par la direction et accessible aux collaborateurs.",
      "estimated_time_minutes": 5,
      "difficulty": "hard",
      "question_code": "ISO27001-A5.1-Q1",
      "chapter": "A.5",
      "evidence_types": ["policy", "evidence", "screenshot"],
      "tags": ["politique", "gouvernance", "ISO 27001"]
    },
    {
      "text": "Combien de comptes administrateurs actifs sont recensÃ©s dans l'Active Directory ?",
      "type": "number",
      "options": null,
      "is_mandatory": true,
      "upload_conditions": null,
      "help_text": "Utiliser PowerShell : Get-ADUser -Filter {Enabled -eq $true -and AdminCount -eq 1} | Measure-Object. Exclure les comptes de service et inclure uniquement les comptes humains.",
      "estimated_time_minutes": 8,
      "difficulty": "hard",
      "question_code": "ISO27001-A9.2-Q1",
      "chapter": "A.9",
      "evidence_types": ["report", "screenshot", "log"],
      "tags": ["contrÃ´le d'accÃ¨s", "comptes privilÃ©giÃ©s"]
    },
    {
      "text": "Quelle est la frÃ©quence des sauvegardes complÃ¨tes des serveurs critiques ?",
      "type": "single_choice",
      "options": ["Quotidienne", "Hebdomadaire", "Mensuelle", "Aucune sauvegarde", "Ne sait pas"],
      "is_mandatory": true,
      "upload_conditions": {
        "required_for_values": ["Quotidienne", "Hebdomadaire"],
        "attachment_types": ["screenshot", "report", "evidence"],
        "min_files": 1,
        "max_files": null,
        "accepts_links": true,
        "help_text": "Joindre une capture d'Ã©cran du planning de sauvegarde OU un rapport de l'outil de backup",
        "is_mandatory": true
      },
      "help_text": "Consulter la planification dans l'outil de sauvegarde (Veeam, Acronis, Backup Exec). VÃ©rifier le planning des tÃ¢ches automatisÃ©es pour les serveurs identifiÃ©s comme critiques.",
      "estimated_time_minutes": 6,
      "difficulty": "medium",
      "question_code": "ISO27001-A12.3-Q1",
      "chapter": "A.12",
      "evidence_types": ["screenshot", "report", "evidence"],
      "tags": ["sauvegarde", "continuitÃ©"]
    },
    {
      "text": "Quelle est la date du dernier test de restauration de sauvegarde rÃ©alisÃ© avec succÃ¨s ?",
      "type": "date",
      "options": null,
      "is_mandatory": false,
      "upload_conditions": null,
      "help_text": "Consulter les comptes-rendus de test dans l'outil de sauvegarde ou les tickets d'intervention. Un test annuel minimum est recommandÃ© par ISO 27001.",
      "estimated_time_minutes": 6,
      "difficulty": "medium",
      "question_code": "ISO27001-A12.3-Q2",
      "chapter": "A.12",
      "evidence_types": ["report", "evidence", "screenshot"],
      "tags": ["sauvegarde", "test"]
    }
  ]
}

âš ï¸ RÃˆGLES JSON STRICTES :
- RÃ©pondre UNIQUEMENT en JSON valide (UTF-8)
- AUCUN texte avant/aprÃ¨s le JSON
- AUCUNE balise markdown (```json)
- AUCUNE balise <think>
- Tous les guillemets doubles (")
- Toutes les virgules correctes
- Tous les crochets/accolades fermÃ©s

âœ… Champs OBLIGATOIRES (NOMS EXACTS Ã€ RESPECTER) :

âš ï¸ ATTENTION : Utiliser EXACTEMENT ces noms de champs (pas "id", pas "requirement_id") :

- "text" : Question claire et prÃ©cise
- "type" : boolean|single_choice|multiple_choice|open|rating|number|date
- "options" : Array pour single_choice/multiple_choice, null sinon
- "is_mandatory" : true (question obligatoire) ou false (optionnelle)
- "upload_conditions" : Object (si preuve requise) ou null (si aucune preuve)
- "help_text" : Guidance technique (commande, chemin fichier, outil Ã  consulter)
- "difficulty" : low|medium|high (selon criticality_level du contrÃ´le)
- "question_code" : Code unique format {FRAMEWORK}-{CHAPTER}-Q{NUMBER} âš ï¸ PAS "id"
- "chapter" : Chapitre/section (ex: "A.5", "A.6") ou null
- "evidence_types" : Array de types de preuves suggÃ©rÃ©s (ex: ["policy", "screenshot"])
- "estimated_time_minutes" : Temps estimÃ© en minutes (2-30)
- "tags" : 1-3 tags pertinents

âš ï¸ NE PAS UTILISER : "id", "requirement_id" - Ces champs ne sont pas utilisÃ©s !

âš ï¸ IMPORTANT UPLOAD_CONDITIONS :
Si upload_conditions n'est pas null, il DOIT contenir :
- "required_for_values" : Array de valeurs dÃ©clenchant l'upload (ex: ["Oui"])
- "attachment_types" : Array de types acceptÃ©s (ex: ["policy", "evidence"])
- "min_files" : Number (dÃ©faut: 1)
- "max_files" : Number ou null (null = illimitÃ©)
- "accepts_links" : Boolean (true pour accepter liens URL)
- "help_text" : String expliquant les preuves acceptÃ©es (texte simple, SANS emojis)
- "is_mandatory" : Boolean (true si upload obligatoire pour conformitÃ©)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ TON OBJECTIF : GÃ©nÃ©rer des questions qu'un auditeur pourrait IMMÃ‰DIATEMENT utiliser
pour collecter des PREUVES VÃ‰RIFIABLES lors d'un audit terrain.

âš ï¸ Si une question ne permet pas de vÃ©rifier/mesurer/prouver quelque chose de concret,
elle n'a PAS sa place dans un questionnaire d'audit professionnel !"""

    # LIGNE 130-160 : REMPLACER __init__

    def __init__(self, db_session: Session):
        self.db = db_session

        # âœ… Lecture robuste avec fallback explicite
        self.ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        self.model = os.getenv("OLLAMA_MODEL", "deepseek-v3.1:671b-cloud")
        
        # âœ… Conversion sÃ©curisÃ©e des types
        try:
            self.temperature = float(os.getenv("DEEPSEEK_TEMPERATURE", "0.6"))
        except ValueError:
            self.temperature = 0.6
            logger.warning("âš ï¸ DEEPSEEK_TEMPERATURE invalide, utilisation de 0.6")
        
        try:
            self.max_tokens = int(os.getenv("DEEPSEEK_MAX_TOKENS", "8192"))
        except ValueError:
            self.max_tokens = 8192
            logger.warning("âš ï¸ DEEPSEEK_MAX_TOKENS invalide, utilisation de 8192")
        
        try:
            self.timeout = int(os.getenv("AI_TIMEOUT_SECONDS", "600"))
        except ValueError:
            self.timeout = 600
            logger.warning("âš ï¸ AI_TIMEOUT_SECONDS invalide, utilisation de 600")
        
        try:
            self.max_retries = int(os.getenv("AI_MAX_RETRIES", "3"))
        except ValueError:
            self.max_retries = 3
        
        try:
            self.batch_size = int(os.getenv("DEEPSEEK_BATCH_SIZE", "10"))
        except ValueError:
            self.batch_size = 10
        
        ai_enabled_str = os.getenv("AI_GENERATION_ENABLED", "true").lower()
        self.ai_enabled = ai_enabled_str in ("true", "1", "yes", "on")

        # âœ… Log de dÃ©marrage dÃ©taillÃ©
        logger.info(
            f"ğŸ¤– [DeepSeek Init] "
            f"URL={self.ollama_url} | "
            f"Model={self.model} | "
            f"Enabled={self.ai_enabled} | "
            f"Timeout={self.timeout}s | "
            f"Retries={self.max_retries} | "
            f"Batch={self.batch_size} | "
            f"Temp={self.temperature} | "
            f"MaxTokens={self.max_tokens}"
        )

    # ======================== Public API ======================== #

    def _chunks(self, seq, n: int):
        """DÃ©coupe une sÃ©quence en lots de n Ã©lÃ©ments."""
        for i in range(0, len(seq), n):
            yield seq[i:i + n]

    def _build_prompt_for_batch(self, items_batch: list) -> str:
        """Construit un prompt concis pour un lot d'exigences d'un rÃ©fÃ©rentiel."""
        lines = [
            f"CONTEXTE : GÃ©nÃ©ration de questions d'audit pour {len(items_batch)} exigences.",
            "Chaque exigence doit donner lieu Ã  plusieurs questions couvrant son intention.",
            "\nEXIGENCES Ã€ COUVRIR :"
        ]
        for r in items_batch:
            code  = r.get("requirement_code") or r.get("official_code") or ""
            title = (r.get("title") or r.get("requirement_title") or "")[:120]
            desc  = (r.get("description") or r.get("requirement_text") or "")[:160]
            dom   = r.get("domain") or "N/A"
            crit  = r.get("criticality_level") or "MEDIUM"  # âœ… RÃ©cupÃ©rer la criticitÃ©

            lines.append(f"[{code}] {title}")
            if desc:
                lines.append(f"  Description : {desc}")
            lines.append(f"  Domaine : {dom}")
            lines.append(f"  CriticitÃ© : {crit}")  # âœ… Informer l'IA de la criticitÃ©

        lines.append(
            """
    INSTRUCTIONS DE SORTIE :
    - RÃ©ponds STRICTEMENT en JSON valide (UTF-8), sans texte avant/aprÃ¨s, sans balises <think>.
    - Toutes les clÃ©s/chaÃ®nes entre doubles guillemets.
    - GÃ©nÃ¨re 5 Ã  10 questions d'audit pratiques par exigence.
    - Types: yes_yes, single_choice, multiple_choice, textarea, number, date.
    - Inclure "help_text" si utile.

    âš ï¸ CRITICITÃ‰ ET DIFFICULTÃ‰ :
    - Utilise la "CriticitÃ©" de chaque exigence pour dÃ©finir "difficulty" :
      â€¢ LOW â†’ difficulty: "low"
      â€¢ MEDIUM â†’ difficulty: "medium"
      â€¢ HIGH â†’ difficulty: "high"
      â€¢ CRITICAL â†’ difficulty: "high"
    - Marque "is_mandatory": true pour les exigences CRITICAL et HIGH

    âš ï¸ IMPORTANT : Suivre le schÃ©ma JSON dÃ©taillÃ© dans SYSTEM_PROMPT ci-dessus.
    Ne PAS utiliser "id" ou "requirement_id" - utiliser "question_code" et "chapter" Ã  la place.
    """.strip()
        )

        prompt = "\n".join(lines)
        return prompt[:8000]
    
    def _rank_cps_for_question(self, question_text: str, candidates: List[Dict[str, Any]]) -> tuple[Optional[Dict[str, Any]], float]:
        """
        Classe des CP candidats pour une question donnÃ©e.
        StratÃ©gie rapide:
        - Score 1: correspondance lexicale (titre/description)
        - Score 2 (optionnel): similaritÃ© embeddings si EmbeddingService dispo
        Retourne (meilleur_cp, score)
        """
        if not candidates:
            return None, -1.0

        q = (question_text or "").lower()
        if not q:
            return candidates[0], 0.0

        # Heuristique lexicale simple
        def lexical_score(cp: Dict[str, Any]) -> float:
            s = f"{cp.get('name','')} {cp.get('description','')}".lower()
            score = 0
            # mini-features
            for term in ["auth", "mfa", "pwd", "backup", "sauvegarde", "journal", "log", "incident", "patch", "vpn", "firewall", "antivirus", "chiffrement", "encrypt"]:
                if term in q and term in s:
                    score += 1.0
            # bonus si mots exacts partagÃ©s (sim. Jaccard simplifiÃ©e)
            qw = set(q.split())
            sw = set(s.split())
            if qw and sw:
                score += len(qw & sw) / max(1, len(qw | sw))
            return score

        ranked = sorted(candidates, key=lexical_score, reverse=True)
        best = ranked[0]
        best_s = lexical_score(best)

        # Si tu veux activer un second Ã©tage plus â€œsmartâ€, branche ton EmbeddingService ici
        # try:
        #     from src.services.embedding_service import EmbeddingService
        #     emb = EmbeddingService()
        #     qv = emb.generate_embedding(question_text)
        #     best_cp, best_score = None, -1.0
        #     for cp in candidates:
        #         sv = emb.generate_embedding(f"{cp.get('name','')} {cp.get('description','')}")
        #         sim = emb.compute_similarity(qv, sv)
        #         if sim > best_score:
        #             best_cp, best_score = cp, sim
        #     return best_cp, best_score
        # except Exception:
        #     pass

        return best, best_s


    def _fetch_control_points_for_requirements(self, requirement_ids: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        """
        Retourne { requirement_id: [ {id, code, name, description, domain} , ... ] }
        en lisant requirement_control_point â†’ control_point.
        """
        from sqlalchemy import text
        if not requirement_ids:
            return {}

        query = text("""
            SELECT
                rcp.requirement_id AS rid,
                cp.id               AS cp_id,
                cp.code             AS cp_code,
                cp.name             AS cp_name,
                cp.description      AS cp_desc,
                cp.category         AS cp_category,
                cp.subcategory      AS cp_subcategory,
                cp.criticality_level AS cp_criticality
            FROM requirement_control_point rcp
            JOIN control_point cp ON cp.id = rcp.control_point_id
            WHERE rcp.requirement_id::text = ANY(:rid_list)
            AND cp.is_active = true
        """)

        rows = self.db.execute(query, {"rid_list": requirement_ids}).mappings().all()
        result: Dict[str, List[Dict[str, Any]]] = {}
        for r in rows:
            rid = str(r["rid"])
            result.setdefault(rid, []).append({
                "id": str(r["cp_id"]),
                "code": r["cp_code"],
                "name": r["cp_name"],
                "description": r["cp_desc"],
                "category": r["cp_category"],
                "subcategory": r["cp_subcategory"],
                "criticality_level": r["cp_criticality"],
            })
        return result

    async def _assign_control_points(self, questions: List[Dict[str, Any]], request) -> List[Dict[str, Any]]:
        """
        Enrichit chaque question avec control_point_id en utilisant:
        1) requirement_control_point (mapping direct)
        2) S'il y a plusieurs PCs possibles pour une exigence: choix par similaritÃ© questionâ†”PC
        3) Fallback: rien (on ne force pas un mauvais mapping)
        """
        if not questions:
            return questions

        # Collecte des exigences rÃ©fÃ©rencÃ©es par les questions
        req_ids: set[str] = set()
        for q in questions:
            for rid in q.get("requirement_ids", []) or []:
                rid_s = str(rid).strip()
                # Valider que c'est un UUID valide (format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)
                if rid_s and len(rid_s) == 36 and rid_s.count('-') == 4:
                    try:
                        # Tenter de valider comme UUID
                        import uuid as uuid_lib
                        uuid_lib.UUID(rid_s)
                        req_ids.add(rid_s)
                    except (ValueError, AttributeError):
                        # Ignorer les IDs invalides
                        pass

        if not req_ids:
            return questions

        # 1) Charger tous les PCs liÃ©s aux exigences (via requirement_control_point)
        cp_by_req: Dict[str, List[Dict[str, Any]]] = self._fetch_control_points_for_requirements(list(req_ids))

        # 2) Pour chaque question, attribuer le meilleur CP (si pas dÃ©jÃ  prÃ©sent)
        out: List[Dict[str, Any]] = []
        for q in questions:
            if q.get("control_point_id"):
                out.append(q)
                continue

            q_text = (q.get("text") or "").strip()
            best_cp = None
            best_score = -1.0

            # Chercher un CP dans l'union des CPs de ses exigences
            candidate_cps: List[Dict[str, Any]] = []
            for rid in q.get("requirement_ids", []) or []:
                rid_s = str(rid).strip()
                candidate_cps.extend(cp_by_req.get(rid_s, []))

            # DÃ©dupliquer par id
            seen = set()
            uniq_candidates = []
            for cp in candidate_cps:
                cid = cp.get("id")
                if cid and cid not in seen:
                    seen.add(cid)
                    uniq_candidates.append(cp)

            if uniq_candidates:
                best_cp, best_score = self._rank_cps_for_question(q_text, uniq_candidates)

            if best_cp:
                q["control_point_id"] = str(best_cp["id"])

            out.append(q)

        return out

    async def _generate_via_deepseek(self, request: QuestionGenerationRequest) -> List[Dict[str, Any]]:
        """
        GÃ©nÃ©ration via DeepSeek (questions brutes) AVEC batching et prompts courts.
        Autonome : ne dÃ©pend plus de _get_source_data().
        """
        all_questions: List[Dict[str, Any]] = []

        # 1) RÃ©cupÃ©ration directe selon le mode
        if request.mode == "framework":
            framework, requirements = self._load_framework_and_requirements(request.framework_id)
            source_type = "framework"
            # RÃ©cupÃ©rer les criticitÃ©s des control points liÃ©s aux requirements
            cp_map = self._fetch_control_points_for_requirements([str(r.id) for r in requirements])

            items = [
                {
                    "anchor_id": str(r.id),
                    "requirement_code": r.official_code,
                    "title": r.title,
                    "requirement_text": (r.requirement_text or "")[:600],
                    "domain": getattr(r, "domain", None),
                    "subdomain": getattr(r, "subdomain", None),
                    # Utiliser la criticitÃ© du premier control point liÃ©, ou "MEDIUM" par dÃ©faut
                    "criticality_level": cp_map.get(str(r.id), [{}])[0].get("criticality_level", "MEDIUM") if cp_map.get(str(r.id)) else "MEDIUM",
                    "official_code": r.official_code,  # Pour extraction du chapter
                }
                for r in requirements
            ]
        elif request.mode == "control_points":
            control_points = self._load_control_points(request.control_point_ids)
            source_type = "control_points"
            items = [
                {
                    "anchor_id": str(cp.id),
                    "code": cp.code,
                    "title": cp.name,
                    "description": (cp.description or "")[:600],
                    "domain": getattr(cp, "category", None) or getattr(cp, "control_family", None),
                    "subdomain": getattr(cp, "subcategory", None),
                    "criticality_level": getattr(cp, "criticality_level", "MEDIUM"),  # CriticitÃ© du CP
                    "official_code": getattr(cp, "code", None),  # Code du CP
                }
                for cp in control_points
            ]
        else:
            raise ValueError("Mode inconnu pour _generate_via_deepseek")

        # 2) Taille de lot (config ou dÃ©faut 10)
        batch_size = int(getattr(self, "batch_size", 10))

        # 3) Boucle par lots
        for batch in self._chunks(items, batch_size):
            
            # APRÃˆS (un seul argument)
            prompt = self._build_prompt_for_batch(batch)

            logger.info(f"[QGen] lot={len(batch)} prompt_chars={len(prompt)}")
            try:
                response_content = await self._call_deepseek_with_retry(prompt)
            except Exception as e:
                logger.error(f"[QGen] Ã‰chec lot ({len(batch)} items) : {e}")
                continue

            # 4) Parsing JSON -> questions
            questions = self._parse_items(response_content)

            # ğŸ” LOG : Afficher un Ã©chantillon de la premiÃ¨re question parsÃ©e
            if questions:
                # Prendre la premiÃ¨re question du batch pour inspection
                first_item = questions[0]
                if isinstance(first_item, dict) and "questions" in first_item:
                    # Format items avec anchor_id
                    sample_questions = first_item.get("questions", [])
                    if sample_questions:
                        sample = sample_questions[0]
                        logger.info(f"ğŸ“‹ [SAMPLE_PARSED] PremiÃ¨re question du lot: {json.dumps(sample, ensure_ascii=False, indent=2)[:500]}...")
                elif isinstance(first_item, dict):
                    # Format direct (liste de questions)
                    logger.info(f"ğŸ“‹ [SAMPLE_PARSED] PremiÃ¨re question du lot: {json.dumps(first_item, ensure_ascii=False, indent=2)[:500]}...")

                all_questions.extend(questions)

        return all_questions




    async def generate_questions(self, request: QuestionGenerationRequest) -> List[GeneratedQuestion]:
        """
        Point d'entrÃ©e unique appelÃ© par l'API.
        DÃ©lÃ¨gue vers la branche adÃ©quate.
        """
        mode = request.mode
        logger.info(f"[QGen] Mode={mode}")

        if mode == "framework":
            return await self._generate_for_framework(request)
        elif mode == "control_points":
            return await self._generate_for_control_points(request)
        else:
            raise ValueError("mode must be 'framework' or 'control_points'")
        

    # ---------------------- MODE: FRAMEWORK --------------------- #
    def _parse_questions(self, response_content: str) -> List[Dict[str, Any]]:
        """
        Parse la rÃ©ponse JSON DeepSeek et renvoie une liste de questions normalisÃ©es.
        TolÃ©rante : accepte des strings/objets partiels et les convertit en dicts utilisables.
        """
        try:
            cleaned = self._clean_json_response(response_content)
        except Exception:
            cleaned = response_content  # dernier recours

        try:
            parsed = json.loads(cleaned)
        except json.JSONDecodeError as e:
            logger.error(f"JSON invalide renvoyÃ© par l'IA: {e}")
            return []

        # On accepte {"questions":[...]} ou directement [...]
        if isinstance(parsed, dict) and "questions" in parsed:
            questions_raw = parsed["questions"]
        elif isinstance(parsed, list):
            questions_raw = parsed
        else:
            logger.warning("RÃ©ponse IA sans clÃ© 'questions' ni liste exploitable")
            return []

        def _normalize_ai_item(item: Any) -> Optional[Dict[str, Any]]:
            # 1) Si string brute â†’ question texte
            if isinstance(item, str):
                txt = item.strip()
                if not txt:
                    return None
                return {
                    "id": str(uuid4()),
                    "text": txt,
                    "type": "text",
                    "options": [],
                    "help_text": "",
                    "difficulty": "medium",
                    "domain": None,
                    "requirement_ids": [],
                    "ai_confidence": 0.8,
                    "rationale": "",
                    "tags": [],
                }

            # 2) Si dict â†’ harmoniser alias + dÃ©fauts
            if isinstance(item, dict):
                out = dict(item)  # shallow copy

                # alias frÃ©quents
                if "question" in out and "text" not in out:
                    out["text"] = out.pop("question")

                # valeurs par dÃ©faut (anciens champs)
                out.setdefault("id", str(uuid4()))
                out.setdefault("text", "")
                out.setdefault("type", "text")
                out.setdefault("options", [])
                out.setdefault("help_text", out.get("rationale", "") or "")
                # âœ… Ne PAS Ã©craser difficulty s'il existe dÃ©jÃ  - setdefault suffit
                out.setdefault("difficulty", "medium")
                out.setdefault("domain", out.get("domain"))
                out.setdefault("requirement_ids", out.get("requirement_ids", []))
                out.setdefault("ai_confidence", float(out.get("ai_confidence", 0.8)))
                out.setdefault("rationale", out.get("rationale", "") or "")
                out.setdefault("tags", out.get("tags", []))

                # âœ… PRÃ‰SERVER les nouveaux champs si prÃ©sents dans la rÃ©ponse IA
                # Ces champs sont maintenant demandÃ©s dans le SYSTEM_PROMPT
                # Ne PAS les Ã©craser avec des valeurs par dÃ©faut
                # out.setdefault("question_code", None)  # â† NE PAS faire Ã§a, prÃ©server si prÃ©sent
                # out.setdefault("chapter", None)
                # out.setdefault("evidence_types", [])
                # out.setdefault("is_mandatory", False)
                # out.setdefault("upload_conditions", None)
                # out.setdefault("estimated_time_minutes", None)

                # types autorisÃ©s
                allowed_types = {
                    "yes_no", "single_choice", "multiple_choice",
                    "text", "textarea", "number", "date", "likert"
                }
                if out["type"] not in allowed_types:
                    out["type"] = "text"

                # options â†’ liste de str
                if not isinstance(out.get("options", []), list):
                    out["options"] = []
                else:
                    out["options"] = [str(o) for o in out["options"] if str(o).strip()]

                # requirement_ids peut Ãªtre str ou liste
                rids = out.get("requirement_ids", [])
                if isinstance(rids, str) and rids.strip():
                    out["requirement_ids"] = [rids.strip()]
                elif isinstance(rids, list):
                    out["requirement_ids"] = [str(x).strip() for x in rids if str(x).strip()]
                else:
                    out["requirement_ids"] = []

                # texte obligatoire
                if not out["text"].strip():
                    return None

                return out

            # 3) Autres types â†’ stringify
            txt = str(item).strip()
            if not txt:
                return None
            return {
                "id": str(uuid4()),
                "text": txt,
                "type": "text",
                "options": [],
                "help_text": "",
                "difficulty": "medium",
                "domain": None,
                "requirement_ids": [],
                "ai_confidence": 0.8,
                "rationale": "",
                "tags": [],
            }

        normalized: List[Dict[str, Any]] = []
        for it in questions_raw if isinstance(questions_raw, list) else []:
            q = _normalize_ai_item(it)
            if isinstance(q, dict) and q.get("text", "").strip():
                normalized.append(q)

        # Appliquer l'enrichissement et la normalisation des champs JSON stringifiÃ©s
        enriched = self._coerce_and_enrich_questions(normalized)

        return enriched

    def _merge_unique_questions(self, q1: List[Dict[str, Any]], q2: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Fusionne deux listes de questions en supprimant les doublons (clÃ© = texte normalisÃ©).
        """
        def norm(s: str) -> str:
            return " ".join((s or "").strip().lower().split())

        seen = set()
        out = []
        for q in (q1 or []):
            t = norm(q.get("text", ""))
            if t and t not in seen:
                seen.add(t)
                out.append(q)
        for q in (q2 or []):
            t = norm(q.get("text", ""))
            if t and t not in seen:
                seen.add(t)
                out.append(q)
        return out

    def _ensure_min_questions(self, questions: List[Dict[str, Any]], reqs: List[Dict[str, Any]], min_count: int = 8) -> List[Dict[str, Any]]:
        """
        Si la gÃ©nÃ©ration IA retourne trop peu de questions, complÃ¨te avec un set algorithmique lÃ©ger
        dÃ©rivÃ© des exigences (patterns standards). Rapide, zÃ©ro appel externe.
        """
        if questions is None:
            questions = []
        if len(questions) >= min_count:
            return questions

        needed = min_count - len(questions)
        # Prendre un petit Ã©chantillon des exigences pour gÃ©nÃ©rer des templates
        base = self._pick_requirement_sample(reqs, max_reqs=min(needed * 2, 12))

        templates = []
        for r in base:
            title = (r.get("title") or "").strip()
            domain = r.get("domain") or None
            rid = r.get("id")
            short = title[:60] if title else "exigence"
            # 5 templates variÃ©s (on en ajoutera autant que nÃ©cessaire)
            templates.extend([
                {
                    "id": str(uuid4()),
                    "text": f"Disposez-vous d'une procÃ©dure formalisÃ©e pour Â« {short} Â» ?",
                    "type": "yes_no",
                    "options": [],
                    "help_text": "ProcÃ©dure documentÃ©e, validÃ©e et diffusÃ©e.",
                    "difficulty": "easy",
                    "domain": domain,
                    "requirement_ids": [rid] if rid else [],
                    "ai_confidence": 0.6,
                    "rationale": "",
                    "tags": []
                },
                {
                    "id": str(uuid4()),
                    "text": f"Quand la derniÃ¨re revue liÃ©e Ã  Â« {short} Â» a-t-elle Ã©tÃ© rÃ©alisÃ©e ?",
                    "type": "date",
                    "options": [],
                    "help_text": "Indiquez la date de la derniÃ¨re revue ou audit interne.",
                    "difficulty": "medium",
                    "domain": domain,
                    "requirement_ids": [rid] if rid else [],
                    "ai_confidence": 0.6,
                    "rationale": "",
                    "tags": []
                },
                {
                    "id": str(uuid4()),
                    "text": f"Quels Ã©lÃ©ments de preuve pouvez-vous fournir concernant Â« {short} Â» ?",
                    "type": "textarea",
                    "options": [],
                    "help_text": "Ex: procÃ©dures, rapports, tickets, journaux.",
                    "difficulty": "medium",
                    "domain": domain,
                    "requirement_ids": [rid] if rid else [],
                    "ai_confidence": 0.6,
                    "rationale": "",
                    "tags": []
                },
                {
                    "id": str(uuid4()),
                    "text": f"Quel est le niveau de mise en Å“uvre actuel pour Â« {short} Â» ?",
                    "type": "single_choice",
                    "options": ["Non dÃ©marrÃ©", "En cours", "Partiellement en place", "Mis en Å“uvre", "OptimisÃ©"],
                    "help_text": "",
                    "difficulty": "easy",
                    "domain": domain,
                    "requirement_ids": [rid] if rid else [],
                    "ai_confidence": 0.6,
                    "rationale": "",
                    "tags": []
                },
                {
                    "id": str(uuid4()),
                    "text": f"Indiquez le nombre d'incidents liÃ©s Ã  Â« {short} Â» sur les 12 derniers mois.",
                    "type": "number",
                    "options": [],
                    "help_text": "Saisir une valeur entiÃ¨re (0 si aucun).",
                    "difficulty": "medium",
                    "domain": domain,
                    "requirement_ids": [rid] if rid else [],
                    "ai_confidence": 0.6,
                    "rationale": "",
                    "tags": []
                },
            ])

        # DÃ©duplication par texte + tronquer au strict nÃ©cessaire
        completed = self._merge_unique_questions(questions, templates)
        return completed[:max(min_count, len(completed))]

    def _pick_requirement_sample(self, reqs: List[Dict[str, Any]], max_reqs: int = 16) -> List[Dict[str, Any]]:
        """
        SÃ©lectionne un Ã©chantillon reprÃ©sentatif rÃ©parti sur toute la liste (dÃ©terministe, sans hasard).
        - RÃ©partit sur toute la longueur (step calculÃ©)
        - ComplÃ¨te par la fin si besoin
        """
        n = len(reqs)
        if n <= max_reqs:
            return list(reqs)

        step = max(1, n // max_reqs)
        sample = []
        idx = 0
        while len(sample) < max_reqs and idx < n:
            sample.append(reqs[idx])
            idx += step

        # ComplÃ©ter si lâ€™arrondi a laissÃ© des â€œtrousâ€
        i = n - 1
        while len(sample) < max_reqs and i >= 0:
            if reqs[i] not in sample:
                sample.append(reqs[i])
            i -= 1

        return sample[:max_reqs]

    def _build_prompt_from_requirements(self, reqs: List[Dict[str, Any]], language: str = "fr") -> str:
        """
        Prompt concis et dÃ©terministe pour DeepSeek Ã  partir d'un Ã©chantillon d'exigences.
        ConÃ§u pour stabilitÃ© JSON et rapiditÃ©. Demande explicitement 8 Ã  12 questions.
        """
        max_reqs = 16  # Ã©chantillon un peu plus large sans exploser les tokens
        sample = self._pick_requirement_sample(reqs, max_reqs=max_reqs)

        lines = []
        lines.append(f"LANGUE: {language}")
        lines.append("MISSION: GÃ©nÃ©rer 8 Ã  12 questions d'audit pratiques, adaptÃ©es PME FR.")
        lines.append("FORMAT: RÃ©pondre STRICTEMENT en JSON valide, sans texte hors JSON.")
        lines.append("SCHEMA:")
        lines.append("""{
    "questions": [
        {
        "text": "...",
        "type": "yes_no|single_choice|multiple_choice|text|textarea|number|date|likert",
        "options": [],
        "help_text": "",
        "difficulty": "easy|medium|hard",
        "domain": "..."
        }
    ]
    }""")
        lines.append("CONSIGNES:")
        lines.append("- Questions claires et opÃ©rationnelles (Ã©viter le blabla)")
        lines.append("- Varier les types (oui/non, choix, texte, date, nombre)")
        lines.append("- Fournir help_text si utile")
        lines.append("- AUCUN markdown, AUCUNE phrase hors JSON")
        lines.append("- Nombre de questions attendu: entre 8 et 12")

        lines.append(f"\nEXIGENCES Ã€ COUVRIR (aperÃ§u, {len(sample)} sur {len(reqs)}) :")
        for r in sample:
            code = r.get("official_code") or r.get("id")
            title = r.get("title") or ""
            desc = (r.get("requirement_text") or "")[:110].replace("\n", " ")
            lines.append(f"- [{code}] {title} | {desc}")

        lines.append("\nRÃ‰PONDS MAINTENANT AVEC UNIQUEMENT LE JSON DEMANDÃ‰ :")
        return "\n".join(lines)

    async def _gather_with_concurrency(self, limit: int, coros_iterable):
        """
        ExÃ©cute des coroutines avec une limite de concurrence.
        """
        import asyncio
        sem = asyncio.Semaphore(max(1, int(limit)))
        results = []

        async def _run(coro):
            async with sem:
                return await coro

        tasks = [asyncio.create_task(_run(c)) for c in coros_iterable]
        for t in tasks:
            try:
                results.append(await t)
            except Exception as e:
                logger.error(f"TÃ¢che concurrente Ã©chouÃ©e: {e}")
                results.append([])
        return results

    
    async def _generate_for_framework(self, request: QuestionGenerationRequest) -> List[GeneratedQuestion]:
        """
        GÃ©nÃ¨re des questions Ã  partir d'un framework.
        Utilise _generate_via_deepseek qui gÃ¨re dÃ©jÃ  le batching et la normalisation.
        """
        # Utiliser la mÃ©thode existante qui fonctionne dÃ©jÃ 
        questions_raw = await self._generate_via_deepseek(request)

        # Aplatir la structure items -> questions
        flat_questions = []
        for item in questions_raw:
            if isinstance(item, dict) and "questions" in item:
                # Structure: {"anchor_id": "...", "questions": [...]}
                anchor_id = item.get("anchor_id", "unknown")
                for q in item.get("questions", []):
                    if q:  # Ignorer les None
                        # Ajouter l'anchor_id/requirement_ids si pas dÃ©jÃ  prÃ©sent
                        if "requirement_ids" not in q and anchor_id != "unknown":
                            q["requirement_ids"] = [anchor_id]
                        elif "requirement_ids" not in q:
                            q["requirement_ids"] = []
                        flat_questions.append(q)
            elif isinstance(item, dict):
                # DÃ©jÃ  une question plate
                flat_questions.append(item)

        # Assigner les control points si nÃ©cessaire
        questions_enriched = await self._assign_control_points(flat_questions, request)

        # Convertir en GeneratedQuestion
        out: List[GeneratedQuestion] = []
        for q in questions_enriched:
            # Extraire requirement_ids
            requirement_ids = q.get("requirement_ids", [])
            if not isinstance(requirement_ids, list):
                requirement_ids = [requirement_ids] if requirement_ids else []

            try:
                generated_q = self._to_generated_question(
                    q,
                    requirement_ids=requirement_ids,
                    control_point_id=q.get("control_point_id")
                )
                out.append(generated_q)
            except Exception as e:
                logger.error(f"Erreur conversion question: {e}")
                continue

        logger.info(f"ğŸ‰ Total : {len(out)} questions gÃ©nÃ©rÃ©es pour le framework")
        return out

    def _build_validation_rules(self, question_data: dict) -> dict:
        """Construit les rÃ¨gles de validation selon le type de rÃ©ponse."""
        q_type = question_data.get("type", "text")
        difficulty = question_data.get("difficulty", "medium")
        
        rules = {}
        
        if q_type == "yes_no":
            rules = {
                "requires_comment_if_no": True,
                "requires_evidence_if_no": True
            }
        
        elif q_type == "single_choice":
            rules = {
                "requires_selection": True,
                "allow_other": False
            }
        
        elif q_type == "multiple_choice":
            rules = {
                "min_selections": 1,
                "max_selections": 10,
                "allow_other": True
            }
        
        elif q_type == "rating":
            rules = {
                "min": 1,
                "max": 5,
                "scale_labels": [
                    "Non implÃ©mentÃ©",
                    "Incomplet", 
                    "Partiel",
                    "Complet",
                    "OptimisÃ©"
                ],
                "requires_comment_if_low": True,
                "low_threshold": 3
            }
        
        elif q_type == "number":
            rules = {
                "min": 0,
                "max": 100,
                "type": "integer",
                "unit": "%"
            }
        
        elif q_type == "date":
            rules = {
                "format": "YYYY-MM-DD",
                "min_date": "2020-01-01",
                "allow_future": False
            }
        
        elif q_type == "open":
            rules = {
                "min_length": 10,
                "max_length": 500,
                "multiline": True
            }
        
        return rules

    def _build_evidence_types(self, question_data: dict) -> list:
        """DÃ©termine les types de preuves selon la difficultÃ©."""
        difficulty = question_data.get("difficulty", "medium")
        
        if difficulty in ["hard", "critical"]:
            return ["document", "screenshot", "policy", "procedure", "audit_report"]
        elif difficulty == "medium":
            return ["document", "screenshot", "policy"]
        elif difficulty in ["easy", "basic"]:  # Support both for backwards compatibility
            return ["document", "screenshot"]
        else:
            return ["document"]

    def _estimate_time(self, question_data: dict) -> int:
        """Estime le temps de rÃ©ponse selon la difficultÃ©."""
        difficulty = question_data.get("difficulty", "medium")
        
        time_map = {
            "easy": 3,
            "basic": 3,  # Backwards compatibility
            "medium": 5,
            "hard": 10,
            "critical": 15
        }
        
        return time_map.get(difficulty, 5)


    def _build_prompt_for_requirement(
        self,
        req: Dict[str, Any],
        language: str = "fr",
        target_count: int = 5,
        alt: bool = False,
    ) -> str:
            """
            Prompt IA compact pour UNE exigence (latence faible).
            alt=True â†’ variante de formulation pour 2e tentative.
            """
            code = req.get("official_code") or req.get("id")
            title = (req.get("title") or "").strip()
            desc = (req.get("requirement_text") or "").strip().replace("\n", " ")
            desc = desc[:350]  # limite tokens

            lines = []
            lines.append(f"LANGUE: {language}")
            lines.append("MISSION: GÃ©nÃ©rer des questions d'audit pour UNE exigence, adaptÃ©es PME FR.")
            lines.append(f"NOMBRE_ATTENDU: {max(1, min(10, int(target_count)))} (Â±1)")
            lines.append("FORMAT: RÃ©pondre STRICTEMENT en JSON valide, sans texte hors JSON.")
            lines.append("SCHEMA: { \"questions\": [ { \"text\":\"...\", \"type\":\"yes_no|single_choice|multiple_choice|text|textarea|number|date|likert\", \"options\":[], \"help_text\":\"\", \"difficulty\":\"easy|medium|hard\", \"domain\":null } ] }")
            lines.append("CONSIGNES:")
            if not alt:
                lines.append("- Questions claires, opÃ©rationnelles, sans jargon inutile")
                lines.append("- Varier les types (oui/non, choix, texte, date, nombre)")
            else:
                lines.append("- PrÃ©fÃ©rer yes_no, single_choice, textarea")
                lines.append("- Limiter la longueur des Ã©noncÃ©s")
            lines.append("- AUCUN markdown, AUCUNE phrase hors JSON")

            lines.append(f"\nEXIGENCE [{code}]: {title}")
            if desc:
                lines.append(f"DESCRIPTION: {desc}")

            lines.append("\nRÃ‰PONDS AVEC UNIQUEMENT LE JSON DEMANDÃ‰ :")
            return "\n".join(lines)


    async def _generate_for_requirement_questions(
        self,
        req: Dict[str, Any],
        language: str = "fr",
        target_count: int = 5,
        min_count: int = 1,
        max_count: int = 10,
    ) -> List[Dict[str, Any]]:
            """
            GÃ©nÃ¨re des questions pour UNE exigence.
            - 1re tentative IA rapide.
            - Si < min_count: 2e tentative IA avec un prompt alternatif.
            - Si encore insuffisant: complÃ©tion algorithmique locale.
            - Tronque Ã  max_count.
            - Ajoute requirement_ids=[rid] systÃ©matiquement.
            """
            rid = str(req.get("id"))
            prompt1 = self._build_prompt_for_requirement(req, language, target_count)
            questions: List[Dict[str, Any]] = []

            try:
                resp1 = await self._call_deepseek_with_retry(prompt1)
                q1 = self._parse_questions(resp1)
                # attacher le rid + official_code + Ã©crÃ©mage
                for q in q1:
                    q.setdefault("requirement_ids", [])
                    if rid not in q["requirement_ids"]:
                        q["requirement_ids"].append(rid)
                    # âœ… Ajouter official_code pour extraction du chapter
                    if "official_code" not in q and req.get("official_code"):
                        q["official_code"] = req.get("official_code")
                questions = q1
            except Exception as e:
                logger.warning(f"[QGen][{rid}] tentative 1 IA Ã©chouÃ©e: {e}")
                questions = []

            # 2e tentative si pas assez
            if len(questions) < min_count:
                prompt2 = self._build_prompt_for_requirement(req, language, target_count, alt=True)
                try:
                    resp2 = await self._call_deepseek_with_retry(prompt2)
                    q2 = self._parse_questions(resp2)
                    for q in q2:
                        q.setdefault("requirement_ids", [])
                        if rid not in q["requirement_ids"]:
                            q["requirement_ids"].append(rid)
                        # âœ… Ajouter official_code pour extraction du chapter
                        if "official_code" not in q and req.get("official_code"):
                            q["official_code"] = req.get("official_code")
                    # dÃ©doublonnage
                    questions = self._merge_unique_questions(questions, q2)
                except Exception as e2:
                    logger.warning(f"[QGen][{rid}] tentative 2 IA Ã©chouÃ©e: {e2}")

            # complÃ©tion algorithmique jusqu'au minimum
            if len(questions) < min_count:
                questions = self._ensure_min_questions(questions, [req], min_count=min_count)

            # plafonner Ã  max_count
            if len(questions) > max_count:
                questions = questions[:max_count]

            return questions


    

    # ------------------- MODE: CONTROL POINTS ------------------- #

    async def _generate_for_control_points(self, request: QuestionGenerationRequest) -> List[GeneratedQuestion]:
        """
        GÃ©nÃ©ration Ã  partir d'une liste de PC.
        Garantit >= 1 question / PC.
        """
        control_points = self._load_control_points(request.control_point_ids)

        target_per_pc = int((request.ai_params or {}).get("target_per_control_point", 2))
        min_per_pc = 1

        anchors = [
            {
                "anchor_id": str(cp.id),
                "code": cp.code,
                "title": cp.name,
                "description": (cp.description or "")[:600],
                "domain": getattr(cp, "category", None) or getattr(cp, "control_family", None),
                "subdomain": getattr(cp, "subcategory", None),
            }
            for cp in control_points
        ]
        prompt = self._build_prompt(anchors, mode="control_points", language=request.language)

        items = await self._ask_or_fallback(anchors, prompt, min_per_anchor=min_per_pc, target_per_anchor=target_per_pc)

        out: List[GeneratedQuestion] = []
        for item in items:
            cpid = item["anchor_id"]
            for q in item["questions"]:
                out.append(self._to_generated_question(q, control_point_id=cpid))

        return out

    # ===================== IA + Fallback ======================== #

    async def _ask_or_fallback(
        self,
        anchors: List[Dict[str, Any]],
        prompt: str,
        min_per_anchor: int,
        target_per_anchor: int,
    ) -> List[Dict[str, Any]]:
        # IA non dispo -> on lÃ¨ve (pas de fallback)
        if not (self.ai_enabled and self.ollama_url):
            raise RuntimeError("IA non disponible ou non configurÃ©e â€“ fallback interdit")

        # IA dispo -> on tente
        response = await self._call_deepseek_with_retry(prompt)
        items = self._parse_items(response)
        # IMPORTANT: pas d'ajout de questions gÃ©nÃ©riques
        return self._enforce_minimums(anchors, items, min_per_anchor, target_per_anchor)



    # --- deepseek_question_generator.py ---




    # ===================== Normalisation JSON =================== #

    # LIGNE 280-350 : REMPLACER _parse_items PAR CETTE VERSION

    def _parse_items(self, raw: str) -> List[Dict[str, Any]]:
        """
        Parse robuste de la rÃ©ponse IA avec 5 stratÃ©gies de rÃ©cupÃ©ration.
        """
        import json
        import re
        
        if not raw or not raw.strip():
            logger.warning("âš ï¸ RÃ©ponse IA vide")
            return []
        
        logger.debug(f"ğŸ“¥ RÃ©ponse brute IA ({len(raw)} chars): {raw[:500]}...")

        # âœ… STRATÃ‰GIE 0 : json-repair (si disponible) - LA PLUS ROBUSTE
        if repair_json:
            try:
                # Nettoyer les balises markdown
                cleaned = raw.strip()
                if cleaned.startswith('```'):
                    first_newline = cleaned.find('\n')
                    if first_newline > 0:
                        cleaned = cleaned[first_newline + 1:]
                if cleaned.endswith('```'):
                    cleaned = cleaned[:-3].strip()

                # RÃ©parer et parser
                repaired = repair_json(cleaned)
                data = json.loads(repaired)
                logger.info(f"âœ… JSON rÃ©parÃ© avec json-repair (stratÃ©gie 0)")

                # Normaliser la structure
                if isinstance(data, dict) and "items" in data:
                    return data["items"]
                elif isinstance(data, dict) and "questions" in data:
                    return [{"anchor_id": "generated", "questions": data["questions"]}]
                elif isinstance(data, list):
                    return data
                return []
            except Exception as e:
                logger.warning(f"âš ï¸ StratÃ©gie 0 (json-repair) Ã©chouÃ©e: {e}")

        # âœ… STRATÃ‰GIE 1 : Extraction JSON entre ```json et ``` (ou tronquÃ©)
        # D'abord essayer avec balises complÃ¨tes
        json_match = re.search(r'```(?:json)?\s*(\{.*\}|\[.*\])\s*```', raw, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                logger.info(f"âœ… JSON extrait des backticks (stratÃ©gie 1)")

                # Normaliser la structure
                if isinstance(data, dict) and "items" in data:
                    return data["items"]
                elif isinstance(data, dict) and "questions" in data:
                    # Convertir en format items
                    return [{"anchor_id": "generated", "questions": data["questions"]}]
                elif isinstance(data, list):
                    return data
                return []
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ StratÃ©gie 1a (backticks complets) Ã©chouÃ©e: {e}")

        # Si Ã§a Ã©choue, essayer avec juste l'ouverture ```json (JSON tronquÃ©)
        json_start = re.search(r'```(?:json)?\s*(\{.*)', raw, re.DOTALL)
        if json_start:
            try:
                json_content = json_start.group(1).strip()
                # Enlever la balise fermante si elle existe
                if json_content.endswith('```'):
                    json_content = json_content[:-3].strip()

                data = json.loads(json_content)
                logger.info(f"âœ… JSON extrait des backticks partiels (stratÃ©gie 1b)")

                if isinstance(data, dict) and "items" in data:
                    return data["items"]
                elif isinstance(data, dict) and "questions" in data:
                    return [{"anchor_id": "generated", "questions": data["questions"]}]
                elif isinstance(data, list):
                    return data
                return []
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ StratÃ©gie 1b (backticks partiels) Ã©chouÃ©e: {e}")
                # Si le JSON est tronquÃ©, passer Ã  la stratÃ©gie de rÃ©cupÃ©ration partielle
                pass
        
        # âœ… STRATÃ‰GIE 2 : Extraction du premier objet/tableau JSON trouvÃ©
        json_object_match = re.search(r'(\{.*\}|\[.*\])', raw, re.DOTALL)
        if json_object_match:
            try:
                data = json.loads(json_object_match.group(1))
                logger.info(f"âœ… JSON trouvÃ© (stratÃ©gie 2)")
                
                if isinstance(data, dict) and "items" in data:
                    return data["items"]
                elif isinstance(data, dict) and "questions" in data:
                    return [{"anchor_id": "generated", "questions": data["questions"]}]
                elif isinstance(data, list):
                    return data
                return []
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ StratÃ©gie 2 Ã©chouÃ©e: {e}")
        
        # âœ… STRATÃ‰GIE 3 : Parse direct aprÃ¨s nettoyage basique
        try:
            cleaned = self._clean_json_response(raw)
            data = json.loads(cleaned)
            logger.info(f"âœ… JSON parsÃ© aprÃ¨s nettoyage (stratÃ©gie 3)")
            
            if isinstance(data, dict) and "items" in data:
                return data["items"]
            elif isinstance(data, dict) and "questions" in data:
                return [{"anchor_id": "generated", "questions": data["questions"]}]
            elif isinstance(data, list):
                return data
            return []
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ StratÃ©gie 3 Ã©chouÃ©e: {e}")
        
        # âœ… STRATÃ‰GIE 4 : Nettoyage agressif
        cleaned = raw.strip()
        
        # Supprimer balises <think>
        cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL)
        
        # Extraire entre { et }
        if '{' in cleaned and '}' in cleaned:
            start_idx = cleaned.find('{')
            end_idx = cleaned.rfind('}') + 1
            cleaned = cleaned[start_idx:end_idx]
            
            try:
                data = json.loads(cleaned)
                logger.info(f"âœ… JSON nettoyÃ© parsÃ© (stratÃ©gie 4)")
                
                if isinstance(data, dict) and "items" in data:
                    return data["items"]
                elif isinstance(data, dict) and "questions" in data:
                    return [{"anchor_id": "generated", "questions": data["questions"]}]
                return []
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ StratÃ©gie 4 Ã©chouÃ©e: {e}")
        
        # âœ… STRATÃ‰GIE 5 : Correction des erreurs courantes
        try:
            # Corriger clÃ©s sans guillemets
            fixed = re.sub(r'([{,]\s*)([A-Za-z0-9_]+)(\s*:)', r'\1"\2"\3', cleaned)
            
            # Remplacer quotes simples
            fixed = fixed.replace("'", '"')
            
            # Fixer virgules doubles
            fixed = re.sub(r',\s*,', ',', fixed)
            
            # Fixer virgules avant ]
            fixed = re.sub(r',\s*\]', ']', fixed)
            
            # Fixer virgules avant }
            fixed = re.sub(r',\s*\}', '}', fixed)
            
            # Supprimer trailing commas
            fixed = re.sub(r',(\s*[}\]])', r'\1', fixed)
            
            data = json.loads(fixed)
            logger.info(f"âœ… JSON corrigÃ© parsÃ© (stratÃ©gie 5)")
            
            if isinstance(data, dict) and "items" in data:
                return data["items"]
            elif isinstance(data, dict) and "questions" in data:
                return [{"anchor_id": "generated", "questions": data["questions"]}]
            elif isinstance(data, list):
                return data
            return []
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Toutes les stratÃ©gies ont Ã©chouÃ©. DerniÃ¨re erreur: {e}")
            logger.error(f"ğŸ“„ Contenu brut (1000 premiers chars):\n{raw[:1000]}")

        # âœ… STRATÃ‰GIE 6 : RÃ©cupÃ©ration partielle (JSON tronquÃ©)
        logger.warning("âš ï¸ Tentative de rÃ©cupÃ©ration partielle du JSON tronquÃ©...")
        try:
            # Retirer les balises markdown si prÃ©sentes
            truncated = raw.strip()
            if truncated.startswith('```'):
                # Trouver la fin de la premiÃ¨re ligne (```json)
                first_newline = truncated.find('\n')
                if first_newline > 0:
                    truncated = truncated[first_newline + 1:]

            # Retirer balise fermante si prÃ©sente
            if truncated.endswith('```'):
                truncated = truncated[:-3].strip()

            logger.debug(f"ğŸ” AprÃ¨s nettoyage markdown, longueur: {len(truncated)}, fin: ...{truncated[-100:]}")

            # Chercher le dÃ©but du tableau de questions
            if '"questions"' in truncated or '"items"' in truncated:
                # Si le JSON se termine mal, essayer de le complÃ©ter
                original_ending = truncated[-50:] if len(truncated) > 50 else truncated
                logger.debug(f"ğŸ” Fin originale du JSON: {original_ending}")

                # Compter les accolades et crochets pour voir si le JSON est fermÃ©
                open_braces = truncated.count('{')
                close_braces = truncated.count('}')
                open_brackets = truncated.count('[')
                close_brackets = truncated.count(']')

                logger.debug(f"ğŸ” Comptage: {{ {close_braces}/{open_braces}, [ {close_brackets}/{open_brackets}")

                # VÃ©rifier si le JSON se termine mal (pas proprement fermÃ© ou tronquÃ© dans une chaÃ®ne)
                ends_properly = truncated.rstrip().endswith('}') or truncated.rstrip().endswith(']')
                is_incomplete_braces = close_braces < open_braces or close_brackets < open_brackets

                # VÃ©rifier si tronquÃ© au milieu d'une chaÃ®ne (nombre impair de guillemets)
                # Note: compter seulement les guillemets qui ne sont pas Ã©chappÃ©s
                unescaped_quotes = len([c for i, c in enumerate(truncated) if c == '"' and (i == 0 or truncated[i-1] != '\\')])
                is_incomplete_string = (unescaped_quotes % 2) != 0

                if is_incomplete_braces or not ends_properly or is_incomplete_string:
                    logger.info(f"ğŸ”§ JSON incomplet dÃ©tectÃ© (braces={is_incomplete_braces}, ends_properly={ends_properly}, incomplete_string={is_incomplete_string}), tentative de complÃ©tion...")

                    # Trouver le dernier objet complet de question
                    # Chercher la derniÃ¨re occurrence de "},\n" ou juste "}"
                    last_complete = truncated.rfind('},')
                    if last_complete == -1:
                        last_complete = truncated.rfind('}')

                    logger.debug(f"ğŸ” DerniÃ¨re accolade complÃ¨te trouvÃ©e Ã  position: {last_complete}")

                    if last_complete > 0:
                        # Couper aprÃ¨s le dernier objet complet
                        truncated = truncated[:last_complete + 1]

                        # VÃ©rifier si on a des guillemets non fermÃ©s aprÃ¨s la coupe
                        unescaped_quotes_after_cut = len([c for i, c in enumerate(truncated) if c == '"' and (i == 0 or truncated[i-1] != '\\')])
                        if (unescaped_quotes_after_cut % 2) != 0:
                            # Fermer la chaÃ®ne de caractÃ¨res ouverte
                            truncated += '"'
                            logger.debug("ğŸ”§ Fermeture de chaÃ®ne de caractÃ¨res ajoutÃ©e")

                        # Fermer proprement le JSON selon la structure attendue
                        # Structure attendue: {"questions": [...]}
                        missing_brackets = open_brackets - truncated.count(']')
                        missing_braces = open_braces - truncated.count('}')

                        completion = ']' * missing_brackets + '}' * missing_braces
                        truncated += completion

                        logger.debug(f"ğŸ”§ Ajout de fermetures: {completion}")
                        logger.debug(f"ğŸ” JSON complÃ©tÃ© (200 derniers chars): ...{truncated[-200:]}")

                        try:
                            data = json.loads(truncated)
                            logger.warning(f"âœ… JSON partiellement rÃ©cupÃ©rÃ© (stratÃ©gie 6)")

                            if isinstance(data, dict) and "items" in data:
                                logger.info(f"âœ… RÃ©cupÃ©rÃ© {len(data['items'])} items partiels")
                                return data["items"]
                            elif isinstance(data, dict) and "questions" in data:
                                logger.info(f"âœ… RÃ©cupÃ©rÃ© {len(data['questions'])} questions partielles")
                                return [{"anchor_id": "generated", "questions": data["questions"]}]
                        except json.JSONDecodeError as parse_err:
                            logger.warning(f"âš ï¸ Parse JSON Ã©chouÃ© aprÃ¨s complÃ©tion: {parse_err}")
                            logger.debug(f"ğŸ” Position erreur: {parse_err.pos if hasattr(parse_err, 'pos') else 'N/A'}")
                            pass
        except Exception as recovery_error:
            logger.warning(f"âš ï¸ RÃ©cupÃ©ration partielle Ã©chouÃ©e: {recovery_error}")
            import traceback
            logger.debug(f"Stack trace: {traceback.format_exc()}")

        # âŒ Ã‰chec total
        raise ValueError(f"JSON totalement invalide aprÃ¨s toutes corrections. Extrait: {raw[:400]}")


    def _normalize_question_dict(self, q: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Nettoie un dict de question et mappe les types vers notre schÃ©ma."""
        text = (q or {}).get("text", "") or ""
        text = text.strip()
        if not text:
            return None

        typ = (q or {}).get("type", "open").strip().lower()
        # Mapping vers nos types
        # - boolean
        # - single_choice
        # - multiple_choice
        # - open
        # - rating
        # - number
        # - date
        if typ in ["yes_no", "boolean", "bool"]:
            typ = "boolean"
        elif typ in ["single", "single_choice", "choice"]:
            typ = "single_choice"
        elif typ in ["multi", "multiple", "multiple_choice", "multi_choice"]:
            typ = "multiple_choice"
        elif typ in ["text", "textarea", "open"]:
            typ = "open"
        elif typ in ["likert", "rating", "scale"]:
            typ = "rating"
        elif typ in ["number", "numeric", "integer", "int"]:
            typ = "number"
        elif typ in ["date", "datetime"]:
            typ = "date"
        else:
            typ = "open"

        options = q.get("options")
        if typ in ["single_choice", "multiple_choice"]:
            if not options or not isinstance(options, list):
                # options minimales de secours
                options = ["Oui", "Partiel", "Non"]
            else:
                options = [str(o).strip() for o in options if str(o).strip()][:12]
                if not options:
                    options = ["Oui", "Partiel", "Non"]
            # For rating, add standard likert if empty
            if typ == "rating" and (not options or len(options) == 0):
                options = ["Non implÃ©mentÃ©", "Incomplet", "Partiel", "Complet", "OptimisÃ©"]
        else:
            options = None

        difficulty = (q.get("difficulty") or "medium").lower()
        if difficulty not in ["easy", "medium", "hard"]:
            difficulty = "medium"

        help_text = q.get("help_text")
        if help_text:
            help_text = str(help_text).strip()
            if len(help_text) > 600:
                help_text = help_text[:600] + "â€¦"

        tags = q.get("tags") or []
        if isinstance(tags, list):
            tags = [str(t).strip() for t in tags if str(t).strip()]
        else:
            tags = []

        # Nouveaux champs pour upload conditions
        is_mandatory = q.get("is_mandatory", False)
        if not isinstance(is_mandatory, bool):
            is_mandatory = False

        upload_conditions = q.get("upload_conditions")
        if upload_conditions and isinstance(upload_conditions, dict):
            # Valider la structure minimale
            if "required_for_values" not in upload_conditions:
                upload_conditions = None
        else:
            upload_conditions = None

        return {
            "text": text,
            "type": typ,
            "options": options,
            "help_text": help_text,
            "difficulty": difficulty,
            "tags": tags,
            "is_mandatory": is_mandatory,
            "upload_conditions": upload_conditions,
        }

    def _enforce_minimums(
        self,
        anchors: List[Dict[str, Any]],
        items: List[Dict[str, Any]],
        min_per_anchor: int,
        target_per_anchor: int,
    ) -> List[Dict[str, Any]]:
        by_id: Dict[str, List[Dict[str, Any]]] = {str(a["anchor_id"]): [] for a in anchors}
        for it in items:
            aid = str(it["anchor_id"])
            if aid in by_id:
                by_id[aid].extend(it["questions"])

        out: List[Dict[str, Any]] = []
        for a in anchors:
            aid = str(a["anchor_id"])
            qs = by_id.get(aid, [])
            # on coupe si trop long, mais on NE complÃ¨te PLUS JAMAIS
            if len(qs) > max(target_per_anchor, min_per_anchor):
                qs = qs[:max(target_per_anchor, min_per_anchor)]
            if qs:
                out.append({"anchor_id": aid, "questions": qs})
            # (option stricte) si tu prÃ©fÃ¨res lever quand < min :
            # else:
            #     raise RuntimeError(f"IA a renvoyÃ© 0 question pour {aid} â€“ fallback interdit")
        return out


    # ===================== Fallback algorithmique ================= #

    def _fallback_generate(
        self,
        anchors: List[Dict[str, Any]],
        min_per_anchor: int,
        target_per_anchor: int,
    ) -> List[Dict[str, Any]]:
        """GÃ©nÃ©ration simple et dÃ©terministe, mais utile et couvrante."""
        items: List[Dict[str, Any]] = []
        for a in anchors:
            qs = self._fallback_questions_for_anchor(a, max(min_per_anchor, target_per_anchor))
            items.append({"anchor_id": str(a["anchor_id"]), "questions": qs})
        return items

    def _fallback_questions_for_anchor(self, anchor: Dict[str, Any], n: int) -> List[Dict[str, Any]]:
        """
        GÃ©nÃ¨re N questions Â« base Â» en se basant sur le titre/description exigence ou PC.
        On varie un peu les types pour Ã©viter lâ€™ennui.
        """
        title = anchor.get("title") or anchor.get("official_code") or anchor.get("code") or "ContrÃ´le"
        desc = anchor.get("requirement_text") or anchor.get("description") or ""

        base = []
        # 1: binaire prÃ©sence
        base.append({
            "text": f"Une politique/procÃ©dure formalisÃ©e existe-t-elle pour Â« {title} Â» ?",
            "type": "boolean",
            "options": None,
            "help_text": f"DÃ©crivez briÃ¨vement le dispositif en place. {desc[:140]}".strip(),
            "difficulty": "easy",
            "tags": ["existence", "policy"]
        })
        # 2: maturitÃ© (rating)
        base.append({
            "text": f"Quel est le niveau de maturitÃ© actuel pour Â« {title} Â» ?",
            "type": "rating",
            "options": ["Non implÃ©mentÃ©", "Incomplet", "Partiel", "Complet", "OptimisÃ©"],
            "help_text": "Ã‰valuez le niveau d'implÃ©mentation actuel.",
            "difficulty": "medium",
            "tags": ["maturity"]
        })
        # 3: preuve (open)
        base.append({
            "text": f"Quelles preuves pouvez-vous fournir pour dÃ©montrer Â« {title} Â» ?",
            "type": "open",
            "options": None,
            "help_text": "Exemples: procÃ©dure, captures d'Ã©cran, export de configuration, rapport",
            "difficulty": "medium",
            "tags": ["evidence"]
        })
        # 4: couverture (single_choice)
        base.append({
            "text": f"Quelle est l'Ã©tendue de couverture de Â« {title} Â» ?",
            "type": "single_choice",
            "options": ["Aucune", "Partielle", "MajoritÃ© des pÃ©rimÃ¨tres", "GÃ©nÃ©ralisÃ©e"],
            "help_text": None,
            "difficulty": "medium",
            "tags": ["coverage"]
        })
        # 5: indicateurs (open)
        base.append({
            "text": f"Quels indicateurs ou mÃ©triques suivez-vous pour Â« {title} Â» ?",
            "type": "open",
            "options": None,
            "help_text": None,
            "difficulty": "hard",
            "tags": ["kpi"]
        })

        # Tronquer/dupliquer de faÃ§on simple pour atteindre n
        out: List[Dict[str, Any]] = []
        i = 0
        while len(out) < n:
            out.append(base[i % len(base)])
            i += 1
        return out

    # ===================== Utils de construction ================= #

    def _build_prompt(self, anchors: List[Dict[str, Any]], mode: str, language: str = "fr") -> str:
        """
        Construit le message utilisateur pour DeepSeek.
        On envoie une liste 'anchors', chacun reprÃ©sentant une exigence (mode framework)
        ou un PC (mode control_points).
        """
        # Exemple concret pour guider l'IA sur le format attendu
        example_question = {
            "text": "L'organisation dispose-t-elle d'une politique de sÃ©curitÃ© approuvÃ©e ?",
            "type": "single_choice",
            "options": ["Oui", "Partiellement", "Non"],
            "is_mandatory": True,
            "difficulty": "hard",  # AdaptÃ© selon criticality_level (LOW=easy, MEDIUM=medium, HIGH/CRITICAL=hard)
            "question_code": "ISO27001-A5.1-Q1",  # Format: {FRAMEWORK}-{CHAPTER}-Q{NUM}
            "chapter": "A.5",  # Extrait de official_code (ex: "A.5.1.1" â†’ "A.5")
            "evidence_types": ["policy", "evidence"],  # Types suggÃ©rÃ©s selon type de question
            "estimated_time_minutes": 5,
            "help_text": "VÃ©rifier dans le rÃ©fÃ©rentiel documentaire",
            "upload_conditions": {
                "required_for_values": ["Oui"],
                "attachment_types": ["policy", "evidence"],
                "min_files": 1,
                "max_files": 2,
                "accepts_links": True,
                "help_text": "Joindre la politique signÃ©e",
                "is_mandatory": True
            },
            "tags": ["politique", "gouvernance"]
        }

        # âš ï¸ PROMPT STRUCTURÃ‰ pour forcer l'IA Ã  gÃ©nÃ©rer tous les champs
        # On envoie les anchors avec leurs mÃ©tadonnÃ©es critiques
        instruction = {
            "task": "GÃ©nÃ©rer des questions d'audit en franÃ§ais",
            "format": "JSON strict",
            "required_fields": {
                "text": "Question claire et prÃ©cise",
                "type": "boolean|single_choice|multiple_choice|open|number|date",
                "options": "Array si choice, null sinon",
                "difficulty": "OBLIGATOIRE - Utiliser anchor.criticality_level : LOW=easy, MEDIUM=medium, HIGH=hard, CRITICAL=hard",
                "question_code": "OBLIGATOIRE - Format ISO27001-{chapter}-Q{num} ex: ISO27001-A5.1-Q1",
                "chapter": "OBLIGATOIRE - Extraire de anchor.official_code ex: A.5.1.1â†’A.5",
                "evidence_types": "OBLIGATOIRE - Array ex: ['policy','evidence'] ou ['screenshot','report']",
                "tags": "OBLIGATOIRE - 2-3 mots-clÃ©s ex: ['politique','SMSI']",
                "is_mandatory": "true si anchor.criticality_level=HIGH|CRITICAL, false sinon",
                "estimated_time_minutes": "3-10 selon complexitÃ©",
                "help_text": "Guidance technique",
                "upload_conditions": "Objet ou null"
            },
            "example": example_question,
            "anchors": anchors[:200]
        }

        return json.dumps(instruction, ensure_ascii=False)

    @staticmethod
    def _clean_json_response(s: str) -> str:
        """Nettoie la rÃ©ponse IA en retirant tout ce qui entoure le JSON."""
        if not s:
            return "{}"
        s = s.strip()

        # Enlever Ã©ventuels blocs <think>...</think> ou balises similaires
        s = re.sub(r"<think>.*?</think>", "", s, flags=re.DOTALL)

        # Enlever Ã©ventuels ```json ... ``` ou ``` ```
        s = re.sub(r"^```(?:json)?", "", s, flags=re.IGNORECASE).strip("` \n")

        # Chercher la premiÃ¨re et la derniÃ¨re accolade valide
        first = s.find("{")
        last = s.rfind("}")

        if first != -1 and last != -1 and last > first:
            cleaned = s[first:last + 1]
        else:
            # Fallback : essayer d'extraire un fragment JSON avec regex
            match = re.search(r"\{.*\}", s, re.DOTALL)
            cleaned = match.group(0) if match else "{}"

        # Supprimer les caractÃ¨res parasites avant/aprÃ¨s
        cleaned = cleaned.strip()

        # Corriger les retours de ligne ou quotes mal Ã©chappÃ©s
        cleaned = cleaned.replace("\n", " ").replace("\r", " ")

        return cleaned


    def _coerce_and_enrich_questions(self, items: list[dict]) -> list[dict]:
        """
        - Normalise les champs (ex: upload_conditions string -> objet)
        - Remplit les valeurs par dÃ©faut attendues par le backend
        - âœ… GÃ©nÃ¨re automatiquement les mÃ©tadonnÃ©es manquantes (question_code, chapter, evidence_types)
        - âœ… Valide et normalise response_type selon la table question_type
        """
        import json

        out: list[dict] = []
        question_counter = 1  # Compteur pour question_code

        for q in items:
            if not isinstance(q, dict):
                continue

            # âœ… NORMALISER LE TYPE AVANT TOUTE CHOSE (pour conformitÃ© FK)
            q = self._normalize_response_type(q)

            # alias Ã©ventuels renvoyÃ©s par le prompt
            if "text" in q and "question_text" not in q:
                q["question_text"] = q["text"]
            if "type" in q and "response_type" not in q:
                q["response_type"] = q["type"]
            if "is_mandatory" in q and "is_required" not in q:
                q["is_required"] = bool(q.get("is_mandatory"))

            # champs obligatoires cÃ´tÃ© DB (voir table public.question)
            q.setdefault("validation_rules", {})
            q.setdefault("help_text", "")
            q.setdefault("difficulty", q.get("difficulty_level", "medium"))
            q.setdefault("estimated_time_minutes", 5)
            q.setdefault("ai_generated", True)
            q.setdefault("created_by", "ai")
            q.setdefault("is_active", True)

            # ğŸ”§ upload_conditions peut arriver en STRING JSON â†’ convertir en OBJET
            uc = q.get("upload_conditions")
            if isinstance(uc, str):
                try:
                    q["upload_conditions"] = json.loads(uc)
                except Exception:
                    logger.warning("[QGen] upload_conditions string non JSON -> ignorÃ©")
                    q["upload_conditions"] = None

            # "tags" peut Ãªtre stringifiÃ© comme "[]"
            tags = q.get("tags")
            if isinstance(tags, str):
                try:
                    q["tags"] = json.loads(tags)
                except Exception:
                    q["tags"] = []

            # Evidence types stringifiÃ©s (rare)
            ev = q.get("evidence_types")
            if isinstance(ev, str):
                try:
                    q["evidence_types"] = json.loads(ev)
                except Exception:
                    q["evidence_types"] = []

            # DifficultÃ© â†’ normaliser pour l'API
            if "difficulty_level" in q and "difficulty" not in q:
                q["difficulty"] = q["difficulty_level"]

            # âœ… FALLBACK: GÃ©nÃ©rer automatiquement les mÃ©tadonnÃ©es si manquantes
            q = self._auto_generate_metadata(q, question_counter)
            question_counter += 1

            out.append(q)

        # ğŸ“Š LOG RÃ‰SUMÃ‰ : Statistiques de normalisation
        if out:
            type_counts = {}
            for q in out:
                rt = q.get("response_type", "unknown")
                type_counts[rt] = type_counts.get(rt, 0) + 1

            logger.info(f"âœ… [COERCE] {len(out)} questions enrichies - RÃ©partition types: {type_counts}")

        return out

    @staticmethod
    def _normalize_response_type(q: dict) -> dict:
        """
        Normalise le champ 'type' ou 'response_type' pour qu'il corresponde
        aux valeurs valides de la table question_type.

        Types valides (codes de question_type):
        - boolean
        - single_choice
        - multiple_choice
        - open
        - number
        - date
        - rating

        Args:
            q: Question dict avec champ 'type' ou 'response_type'

        Returns:
            Question avec type normalisÃ©
        """
        # RÃ©cupÃ©rer le type depuis 'type' ou 'response_type'
        original_type = q.get("type") or q.get("response_type") or "open"
        typ = original_type.strip().lower()

        # Mapping des variantes vers les codes valides
        if typ in ["yes_no", "boolean", "bool", "yes/no", "oui/non"]:
            normalized = "boolean"
        elif typ in ["single", "single_choice", "choice", "radio"]:
            normalized = "single_choice"
        elif typ in ["multi", "multiple", "multiple_choice", "multi_choice", "checkbox"]:
            normalized = "multiple_choice"
        elif typ in ["text", "textarea", "open", "texte", "libre"]:
            normalized = "open"
        elif typ in ["likert", "rating", "scale", "Ã©chelle", "notation"]:
            normalized = "rating"
        elif typ in ["number", "numeric", "integer", "int", "nombre"]:
            normalized = "number"
        elif typ in ["date", "datetime", "calendar"]:
            normalized = "date"
        else:
            # Fallback : si type inconnu, on met "open" (texte libre)
            logger.warning(f"âš ï¸ [TYPE_NORMALIZATION] Type inconnu '{original_type}' â†’ normalisÃ© en 'open' (question: {q.get('text', 'N/A')[:50]}...)")
            normalized = "open"

        # ğŸ” LOG : Afficher uniquement si normalisation effectuÃ©e
        if normalized != typ and typ in ["text", "single", "multi", "textarea", "yes_no", "checkbox", "radio"]:
            logger.info(f"âœ… [TYPE_NORMALIZATION] '{original_type}' â†’ '{normalized}' (question: {q.get('text', 'N/A')[:50]}...)")

        # Mettre Ã  jour les deux champs pour cohÃ©rence
        q["type"] = normalized
        q["response_type"] = normalized

        return q

    def _auto_generate_metadata(self, q: dict, counter: int) -> dict:
        """
        GÃ©nÃ¨re automatiquement les mÃ©tadonnÃ©es manquantes (fallback).

        Args:
            q: Question dict
            counter: NumÃ©ro de question pour gÃ©nÃ©ration du code

        Returns:
            Question enrichie
        """
        # 1. GÃ©nÃ©rer question_code si manquant
        if not q.get("question_code"):
            req_code = q.get("requirement_code") or q.get("official_code")
            if req_code:
                chapter = self._extract_chapter_from_code(req_code)
                q["question_code"] = f"ISO27001-{chapter}-Q{counter}" if chapter else f"CUSTOM-GEN-Q{counter}"
            else:
                q["question_code"] = f"CUSTOM-GEN-Q{counter}"

        # 2. GÃ©nÃ©rer chapter si manquant
        if not q.get("chapter"):
            req_code = q.get("requirement_code") or q.get("official_code")
            if req_code:
                q["chapter"] = self._extract_chapter_from_code(req_code)

        # 3. GÃ©nÃ©rer evidence_types si vide
        if not q.get("evidence_types") or (isinstance(q.get("evidence_types"), list) and len(q["evidence_types"]) == 0):
            q["evidence_types"] = self._generate_evidence_types(
                question_type=q.get("type") or q.get("response_type", "open"),
                difficulty=q.get("difficulty", "medium")
            )

        return q

    @staticmethod
    def _extract_chapter_from_code(official_code: str) -> Optional[str]:
        """
        Extrait le chapitre depuis un code officiel.

        Exemples:
        - "A.5.1.1" â†’ "A.5"
        - "A.6.2.1" â†’ "A.6"
        - "5.1.2" â†’ "5.1"
        """
        if not official_code:
            return None

        code = str(official_code).strip()

        if "." in code:
            parts = code.split(".")
            if len(parts) >= 2:
                return f"{parts[0]}.{parts[1]}"

        return None

    @staticmethod
    def _generate_evidence_types(question_type: str, difficulty: str) -> list[str]:
        """
        GÃ©nÃ¨re les types de preuves suggÃ©rÃ©s selon le type et la difficultÃ©.

        Args:
            question_type: Type de question
            difficulty: Niveau de difficultÃ©

        Returns:
            Liste des types de preuves
        """
        # Mapping type â†’ evidence_types par dÃ©faut
        type_mapping = {
            "boolean": ["policy", "evidence", "screenshot"],
            "single_choice": ["screenshot", "report", "evidence"],
            "multiple_choice": ["screenshot", "report", "evidence"],
            "open": ["policy", "evidence", "screenshot"],
            "number": ["report", "screenshot", "log"],
            "date": ["report", "evidence", "screenshot"],
            "rating": ["evidence", "report"]
        }

        base_types = type_mapping.get(question_type.lower(), ["document", "evidence"])

        # Enrichir selon difficultÃ©
        difficulty_lower = (difficulty or "medium").lower()

        if difficulty_lower in ["hard", "high", "critical"]:
            # Questions difficiles â†’ plus de types de preuves
            additional = ["audit_report", "procedure"]
            for t in additional:
                if t not in base_types:
                    base_types.append(t)

        return base_types


    def _to_generated_question(
        self,
        q: Dict[str, Any],
        requirement_ids: Optional[List[str]] = None,
        control_point_id: Optional[str] = None,
    ) -> GeneratedQuestion:
        """Transforme un dict de question normalisÃ© â†’ GeneratedQuestion."""
        text = q.get("text", "")
        if not text:
            raise ValueError("Question text is required")

        typ = q.get("type", "open")
        # Map vers nos types (schemas.questionnaire)
        # - boolean
        # - single_choice
        # - multiple_choice
        # - open
        # - rating
        # - number
        # - date
        mapped_type = {
            "boolean": "boolean",
            "single_choice": "single_choice",
            "multiple_choice": "multiple_choice",
            "open": "open",
            "rating": "rating",
            "number": "number",
            "date": "date",
        }.get(typ, "open")

        return GeneratedQuestion(
            id=str(uuid4()),
            text=text,
            type=mapped_type,  # Literal acceptÃ© par notre schÃ©ma
            options=q.get("options"),
            control_point_id=control_point_id,
            requirement_ids=requirement_ids or [],
            difficulty=q.get("difficulty"),
            ai_confidence=q.get("ai_confidence"),
            rationale=q.get("rationale"),
            help_text=q.get("help_text"),  # âœ… Aide contextuelle pour l'auditÃ© (DISTINCT de rationale)
            tags=q.get("tags", []),
            is_mandatory=q.get("is_mandatory", False),
            upload_conditions=q.get("upload_conditions"),
        )

    # ===================== Chargements BDD ======================= #

    def _load_framework_and_requirements(self, framework_id: Optional[str]) -> Tuple[Framework, List[Requirement]]:
        if not framework_id:
            raise ValueError("framework_id is required for mode 'framework'")

        fw = self.db.query(Framework).filter_by(id=framework_id, is_active=True).first()
        if not fw:
            raise ValueError("Framework not found or inactive")

        # RÃ©cupÃ©rer toutes les exigences actives pour ce framework
        reqs = self.db.execute(
            text(
                """
                SELECT id, official_code, title, requirement_text, domain_id, -- modÃ¨le 'audit' utilise domain_id
                       NULL::text as domain, NULL::text as subdomain
                FROM requirement
                WHERE framework_id = :fid AND is_active = true
                ORDER BY official_code NULLS LAST, created_at
                """
            ),
            {"fid": str(fw.id)},
        ).mappings().all()

        # Si tu stockes 'domain' / 'subdomain' en colonnes texte dans ton autre modÃ¨le,
        # tu peux adapter la sÃ©lection. Ici on reste compatible avec le dump fourni.

        # Convertir en pseudo-objets Requirement via SQLAlchemy si besoin
        # mais ici on renvoie les Mapping rows (dict-like) â€“ suffisant pour le prompt.
        # Pour la conformitÃ© des types attendus par _to_generated_question,
        # on a seulement besoin des IDs.

        # Par cohÃ©rence, on fabrique des "objets" lÃ©gers avec attributs :
        class RWrap:
            def __init__(self, row):
                self.id = row["id"]
                self.official_code = row["official_code"]
                self.title = row["title"]
                self.requirement_text = row["requirement_text"]
                self.domain = row["domain"]
                self.subdomain = row["subdomain"]

        requirements = [RWrap(r) for r in reqs]
        return fw, requirements

    def _load_control_points(self, cp_ids: Optional[List[str]]) -> List[ControlPoint]:
        if not cp_ids:
            raise ValueError("control_point_ids is required for mode 'control_points'")

        rows = self.db.execute(
            text(
                """
                SELECT id, code, name, description, category, subcategory, control_family
                FROM control_point
                WHERE id::text = ANY(:ids)
                """
            ),
            {"ids": cp_ids},
        ).mappings().all()

        class CPWrap:
            def __init__(self, row):
                self.id = row["id"]
                self.code = row["code"]
                self.name = row["name"]
                self.description = row["description"]
                self.category = row["category"]
                self.subcategory = row["subcategory"]
                self.control_family = row["control_family"]

        return [CPWrap(r) for r in rows]
    
    def _fallback_generate(self, *args, **kwargs):
        raise RuntimeError("Fallback algorithmique dÃ©sactivÃ©")

    def _fallback_questions_for_anchor(self, *args, **kwargs):
        raise RuntimeError("Fallback algorithmique dÃ©sactivÃ©")
    

    # LIGNE 730-800 : AMÃ‰LIORER _call_deepseek_with_retry

    # LIGNE 1080-1150 : AMÃ‰LIORER _call_deepseek_with_retry

    async def _call_deepseek_with_retry(self, prompt: str) -> str:
        """
        Appel du modÃ¨le DeepSeek (via Ollama/OpenAI-compatible) avec:
        - messages = [system, user]
        - retries + backoff
        - timeouts progressifs
        - gestion 5xx/502 Bad Gateway
        - support multi-endpoints (Ollama + OpenAI-like)
        """
        if not self.ollama_url:
            raise RuntimeError("ollama_url non configurÃ©e")

        # Chemin d'API : supporte /api/chat (Ollama 0.1+) et /v1/chat/completions (OpenAI-like)
        # on tente d'abord /api/chat (Ollama), sinon fallback OpenAI-like.
        endpoints = [
            f"{self.ollama_url.rstrip('/')}/api/chat",
            f"{self.ollama_url.rstrip('/')}/v1/chat/completions",
        ]

        def build_payload(is_openai: bool) -> Dict[str, Any]:
            messages = [
                {"role": "system", "content": self.SYSTEM_PROMPT},
                {"role": "user", "content": prompt},
            ]
            if not is_openai:
                # Ollama chat endpoint
                return {
                    "model": self.model,
                    "messages": messages,
                    "format": "json",
                    "stream": False,
                    "keep_alive": "5m",
                    "options": {
                        "temperature": self.temperature,
                        "top_p": 0.9,
                        "num_predict": self.max_tokens,
                        "repeat_penalty": 1.1,
                    },
                }
            # OpenAI-like
            return {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "top_p": 0.9,
                "stream": False,
                "max_tokens": self.max_tokens,
                "response_format": {"type": "json_object"},
            }

        def extract_text(resp_json: Dict[str, Any], is_openai: bool) -> str:
            # Ollama /api/chat -> {"message": {"content": "..."}}
            if not is_openai:
                return resp_json.get("message", {}).get("content", "")
            # OpenAI-like -> {"choices":[{"message":{"content":"..."}}]}
            choices = resp_json.get("choices") or []
            if choices and "message" in choices[0]:
                return choices[0]["message"].get("content", "")
            # certains serveurs renvoient directement "content"
            return resp_json.get("content", "")

        last_error = None
        for attempt in range(1, self.max_retries + 1):
            for idx, ep in enumerate(endpoints):
                is_openai = ep.endswith("/v1/chat/completions")
                try:
                    base_timeout = 120  # base par tentative
                    timeout_seconds = base_timeout * attempt
                    timeout = httpx.Timeout(connect=30.0, read=float(timeout_seconds), write=30.0, pool=30.0)

                    payload = build_payload(is_openai)
                    logger.debug(f"â¡ï¸ POST {ep} (try {attempt}/{self.max_retries})")

                    async with httpx.AsyncClient(timeout=timeout, verify=True) as client:
                        r = await client.post(ep, json=payload)
                        if r.status_code >= 500:
                            raise RuntimeError(f"Upstream {r.status_code}: {r.text}")
                        if r.status_code == 404 and idx == 0:
                            # /api/chat absent -> tenter fallback OpenAI-like
                            logger.info("â„¹ï¸ Endpoint /api/chat introuvable, essai OpenAI-like...")
                            continue
                        r.raise_for_status()

                        data = r.json()
                        content = extract_text(data, is_openai)
                        if not content:
                            # certains serveurs renvoient 'response' ou 'message'
                            content = data.get("response") or data.get("message") or ""

                        if not content.strip():
                            raise RuntimeError("RÃ©ponse DeepSeek vide")

                        logger.debug(f"âœ… RÃ©ponse IA reÃ§ue ({len(content)} chars)")
                        return content

                except (httpx.ReadTimeout, httpx.ConnectTimeout) as e:
                    last_error = e
                    logger.warning(f"â³ Timeout (tentative {attempt}) sur {ep}: {e}")
                except httpx.HTTPError as e:
                    last_error = e
                    code = getattr(e.response, "status_code", "N/A")
                    body = getattr(e.response, "text", "")
                    logger.error(f"âŒ HTTPError {code} sur {ep}: {body[:500]}")
                    # 4xx: ne pas rÃ©essayer sur ce endpoint, passer au suivant ou prochaine tentative
                except Exception as e:
                    last_error = e
                    logger.error(f"âŒ Exception appel IA ({type(e).__name__}): {e}")

            # Backoff exponentiel
            await asyncio.sleep(min(2 ** attempt, 10))

        raise RuntimeError(f"Ã‰chec appel DeepSeek aprÃ¨s {self.max_retries} tentatives: {last_error}")


    async def _call_deepseek_generate(self, prompt: str) -> str:
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.temperature,
                "top_p": 0.9,
                "num_predict": self.max_tokens,
            },
        }
        async with httpx.AsyncClient(timeout=httpx.Timeout(self.timeout)) as client:
            url = f"{self.ollama_url.rstrip('/')}/api/generate"
            resp = await client.post(url, json=payload)
            resp.raise_for_status()
            data = resp.json()
            if "response" in data:
                return data["response"]
            raise ValueError("Structure de rÃ©ponse inconnue pour /api/generate")

    # --- Ajoutez ces 2 fonctions utilitaires dans la classe DeepSeekQuestionGenerator ---

def _normalize_ai_item(self, item: Any) -> Dict[str, Any]:
    """TolÃ©rant: transforme str/objets 'bizarres' en dict question standard."""
    if isinstance(item, str):
        return {"text": item.strip(), "type": "text", "options": [], "help_text": "", "difficulty": "medium"}
    if isinstance(item, dict):
        # Harmoniser quelques alias frÃ©quents
        if "question" in item and "text" not in item:
            item["text"] = item.pop("question")
        item.setdefault("text", "")
        item.setdefault("type", "text")
        item.setdefault("options", [])
        item.setdefault("help_text", item.get("rationale", ""))
        item.setdefault("difficulty", item.get("difficulty", "medium"))
        return item
    # Dernier recours
    return {"text": str(item).strip(), "type": "text", "options": [], "help_text": "", "difficulty": "medium"}

def _normalize_ai_questions(self, raw_list: Any) -> List[Dict[str, Any]]:
    if not isinstance(raw_list, list):
        raw_list = [raw_list] if raw_list is not None else []
    return [self._normalize_ai_item(x) for x in raw_list]
