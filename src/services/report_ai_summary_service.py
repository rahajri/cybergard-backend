"""
Service de gÃ©nÃ©ration de rÃ©sumÃ©s IA pour les rapports d'audit.

GÃ©nÃ¨re des rÃ©sumÃ©s exÃ©cutifs en utilisant DeepSeek/Ollama avec diffÃ©rents
tons adaptÃ©s aux publics cibles (Direction, RSSI, Auditeurs).
"""

from typing import Dict, Any, List, Optional
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import text
import httpx
import os
import logging

logger = logging.getLogger(__name__)


class ReportAISummaryService:
    """GÃ©nÃ¨re des rÃ©sumÃ©s IA pour les rapports d'audit."""

    # ========================================================================
    # PROMPT SYSTÃˆME (Base commune) - FORMAT JSON OBLIGATOIRE
    # ========================================================================
    SYSTEM_PROMPT = """Tu es un expert senior en cybersÃ©curitÃ© et conformitÃ© ISO 27001 avec 15 ans d'expÃ©rience en audit.

MISSION : Analyser les rÃ©sultats d'un audit de cybersÃ©curitÃ© et gÃ©nÃ©rer un rÃ©sumÃ© exÃ©cutif professionnel.

âš ï¸ INSTRUCTION CRITIQUE - FORMAT JSON OBLIGATOIRE :
Tu dois TOUJOURS rÃ©pondre avec un objet JSON valide contenant le rÃ©sumÃ©.
Format de rÃ©ponse OBLIGATOIRE :
{
  "summary": "Le contenu complet du rÃ©sumÃ© ici..."
}

RÃˆGLES ABSOLUES :
- Maximum 400 mots dans le rÃ©sumÃ©
- FranÃ§ais formel et professionnel
- Chiffres prÃ©cis (pourcentages, nombres exacts)
- Pas de formules de politesse ni d'introduction gÃ©nÃ©rique
- Pas de jargon technique excessif sauf si ton "technical"
- Ton factuel, direct et orientÃ© dÃ©cision
- Structure claire avec sections visuellement distinctes
- Utiliser les emojis âœ… âš ï¸ ğŸ”´ ğŸ¯ ğŸ“… pour la lisibilitÃ©

FORMAT DU RÃ‰SUMÃ‰ (dans le champ "summary") :
- Texte structurÃ© prÃªt Ã  Ãªtre insÃ©rÃ© dans un PDF
- Pas de markdown (pas de ** ou ## ou ```)
- Listes Ã  puces avec le caractÃ¨re â€¢
- Sections sÃ©parÃ©es par \\n\\n (double saut de ligne)
"""

    # ========================================================================
    # PROMPTS PAR TON - RAPPORT CONSOLIDÃ‰
    # ========================================================================
    CONSOLIDATED_PROMPTS = {
        "executive": """
CONTEXTE : Rapport consolidÃ© multi-organismes pour Direction gÃ©nÃ©rale / CODIR

PUBLIC CIBLE : PDG, DAF, membres du comitÃ© de direction
Ils veulent : Vue stratÃ©gique, risques business, ROI, dÃ©cisions Ã  prendre

STRUCTURE OBLIGATOIRE :

VUE D'ENSEMBLE (2-3 phrases)
RÃ©sumer la portÃ©e de l'audit et le niveau de maturitÃ© global de l'Ã©cosystÃ¨me.
Mentionner le nombre d'organismes, le taux de conformitÃ© moyen, et le positionnement.

âœ… POINTS FORTS (3-4 bullets)
Identifier les domaines oÃ¹ les investissements portent leurs fruits.
Focus sur ce qui fonctionne et protÃ¨ge l'entreprise.

âš ï¸ RISQUES STRATÃ‰GIQUES (3-4 bullets)
Identifier les risques business majeurs (pas techniques).
Formuler en termes d'impact : "Exposition Ã ...", "Risque de...", "VulnÃ©rabilitÃ© face Ã ..."

ğŸ¯ RECOMMANDATIONS PRIORITAIRES (3 actions max)
Actions concrÃ¨tes avec estimation budgÃ©taire et timeline.
Format : [Action] ([Trimestre], [Budget estimÃ©])
""",

        "technical": """
CONTEXTE : Rapport consolidÃ© multi-organismes pour Ã©quipes techniques

PUBLIC CIBLE : RSSI, DSI, Responsables sÃ©curitÃ©, Ã‰quipes IT
Ils veulent : DÃ©tails techniques, mesures concrÃ¨tes, indicateurs prÃ©cis, plan d'action opÃ©rationnel

STRUCTURE OBLIGATOIRE :

SYNTHÃˆSE TECHNIQUE (2-3 phrases)
PÃ©rimÃ¨tre technique Ã©valuÃ©, nombre de contrÃ´les, score de maturitÃ© selon Ã©chelle 1-5.

âœ… CONFORMITÃ‰S TECHNIQUES (4-5 bullets)
ContrÃ´les bien implÃ©mentÃ©s avec niveau de dÃ©tail technique.
Mentionner les outils, configurations, processus en place.

ğŸ”´ NON-CONFORMITÃ‰S TECHNIQUES (4-5 bullets)
Gaps identifiÃ©s avec prÃ©cision technique.
Format : [Domaine] : [ProblÃ¨me technique prÃ©cis]

ğŸ¯ PLAN D'ACTION TECHNIQUE (4-5 actions)
Mesures techniques prioritaires avec effort estimÃ©.
Format : [Action technique] ([DurÃ©e estimÃ©e])

ğŸ“Š INDICATEURS CLÃ‰S Ã€ SUIVRE
MÃ©triques techniques Ã  suivre.
""",

        "detailed": """
CONTEXTE : Rapport consolidÃ© multi-organismes pour experts conformitÃ©

PUBLIC CIBLE : Auditeurs, Consultants GRC, Experts conformitÃ©, Juristes
Ils veulent : Analyse mÃ©thodologique, dÃ©tails par clause ISO, observations qualitatives, recommandations normatives

STRUCTURE OBLIGATOIRE :

CONTEXTE D'AUDIT (3-4 phrases)
MÃ©thodologie appliquÃ©e, pÃ©rimÃ¨tre, Ã©chantillonnage, rÃ©fÃ©rentiels croisÃ©s, limitations.

ğŸ“Š RÃ‰SULTATS PAR CLAUSE ISO 27001 (tableau ou liste)
Analyse statistique par annexe A avec variance inter-organismes.

ğŸ” OBSERVATIONS MÃ‰THODOLOGIQUES (3-4 bullets)
QualitÃ© des preuves, cohÃ©rence des rÃ©ponses, points d'attention audit.

ğŸ“‹ ANALYSE DES Ã‰CARTS (par criticitÃ©)
Ã‰carts critiques, majeurs, mineurs avec rÃ©fÃ©rences normatives.

ğŸ¯ RECOMMANDATIONS NORMATIVES
Actions classÃ©es par horizon temporel avec rÃ©fÃ©rence aux clauses ISO.
"""
    }

    # ========================================================================
    # PROMPTS PAR TON - RAPPORT INDIVIDUEL
    # ========================================================================
    INDIVIDUAL_PROMPTS = {
        "executive": """
CONTEXTE : Rapport individuel pour Direction de l'organisme auditÃ©

PUBLIC CIBLE : Direction gÃ©nÃ©rale de l'organisme auditÃ©
Ils veulent : OÃ¹ ils en sont, comment ils se comparent, quoi faire en prioritÃ©

STRUCTURE OBLIGATOIRE :

POSITIONNEMENT (2-3 phrases)
Score global, niveau de maturitÃ©, position vs pairs et vs secteur.
Ã‰volution par rapport Ã  l'audit prÃ©cÃ©dent si disponible.

âœ… ATOUTS Ã€ CAPITALISER (3-4 bullets)
Ce qui fonctionne bien et doit Ãªtre maintenu/valorisÃ©.

âš ï¸ AXES D'AMÃ‰LIORATION PRIORITAIRES (3-4 bullets)
Domaines nÃ©cessitant investissement, formulÃ©s en termes business.

ğŸ¯ FEUILLE DE ROUTE (3-4 Ã©tapes)
Plan d'action sÃ©quencÃ© avec jalons clairs.

ğŸ’° INVESTISSEMENT RECOMMANDÃ‰
Estimation budgÃ©taire globale et ROI attendu.
""",

        "technical": """
CONTEXTE : Rapport individuel pour Ã©quipe technique de l'organisme

PUBLIC CIBLE : DSI, RSSI, Ã‰quipe IT de l'organisme auditÃ©
Ils veulent : DÃ©tails techniques prÃ©cis, quoi corriger comment

STRUCTURE OBLIGATOIRE :

Ã‰TAT DES LIEUX TECHNIQUE (2-3 phrases)
Score par domaine technique, points de contrÃ´le Ã©valuÃ©s, niveau CMMI/maturitÃ©.

âœ… CONTRÃ”LES CONFORMES (5-6 bullets)
Mesures techniques en place et efficaces.
Mentionner outils, configurations, versions.

ğŸ”´ Ã‰CARTS TECHNIQUES Ã€ CORRIGER (5-6 bullets)
Non-conformitÃ©s avec dÃ©tail technique prÃ©cis.
Format : [ContrÃ´le] : [Ã‰tat actuel] â†’ [Ã‰tat cible]

ğŸ¯ PLAN DE REMÃ‰DIATION TECHNIQUE (5-6 actions)
Actions techniques ordonnÃ©es par prioritÃ© et dÃ©pendance.
Format : [Action] | Effort : [J/H] | PrÃ©requis : [X]

ğŸ“Š MÃ‰TRIQUES CIBLES
KPIs techniques Ã  atteindre.
""",

        "detailed": """
CONTEXTE : Rapport individuel dÃ©taillÃ© pour responsable conformitÃ©

PUBLIC CIBLE : Responsable conformitÃ©, DPO, Consultant GRC de l'organisme
Ils veulent : Analyse exhaustive, mapping normatif, plan de certification

STRUCTURE OBLIGATOIRE :

ANALYSE DE MATURITÃ‰ (3-4 phrases)
Positionnement sur Ã©chelle de maturitÃ© (Initial/RÃ©pÃ©table/DÃ©fini/GÃ©rÃ©/OptimisÃ©).
Benchmark dÃ©taillÃ© par domaine vs rÃ©fÃ©rentiel et vs pairs.

ğŸ“Š CARTOGRAPHIE CONFORMITÃ‰
Analyse dÃ©taillÃ©e par domaine ISO avec taux et tendance.

ğŸ” ANALYSE DES PREUVES
QualitÃ© et complÃ©tude de la documentation fournie.
Ã‰carts entre dÃ©claratif et preuves.

ğŸ“‹ REGISTRE DES Ã‰CARTS
Liste exhaustive classÃ©e par criticitÃ© avec rÃ©fÃ©rence normative.

ğŸ¯ TRAJECTOIRE DE CERTIFICATION
Roadmap dÃ©taillÃ©e vers certification ISO 27001 si applicable.
Jalons, prÃ©requis, effort estimÃ©.

ğŸ“… PLAN D'ACTION DÃ‰TAILLÃ‰
Actions par trimestre avec responsable suggÃ©rÃ© et livrables.
"""
    }

    def __init__(self, db: Session):
        self.db = db
        self.ollama_url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        # Utilise OLLAMA_MODEL_ADVANCED car OLLAMA_MODEL peut Ãªtre GLM qui ne fonctionne pas bien
        self.model = os.getenv("OLLAMA_MODEL_ADVANCED", "deepseek-v3.1:671b-cloud")
        logger.info(f"ğŸ¤– ReportAISummaryService initialisÃ© avec modÃ¨le: {self.model}")

    # ========================================================================
    # MÃ‰THODES PUBLIQUES
    # ========================================================================

    async def generate_campaign_summary(
        self,
        campaign_id: UUID,
        tenant_id: UUID,
        tone: str = "executive",
        language: str = "fr"
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re un rÃ©sumÃ© consolidÃ© pour une campagne entiÃ¨re.

        Args:
            campaign_id: ID de la campagne
            tenant_id: ID du tenant
            tone: executive, technical, detailed
            language: fr, en

        Returns:
            Dict avec executive_summary, key_findings, top_recommendations, statistics
        """
        logger.info(f"ğŸ¤– GÃ©nÃ©ration rÃ©sumÃ© campagne {campaign_id} - Ton: {tone}")

        # 1. Collecter les donnÃ©es
        campaign_data = self._collect_campaign_data(campaign_id, tenant_id)

        # 2. Construire le prompt
        system_prompt = self.SYSTEM_PROMPT + self.CONSOLIDATED_PROMPTS.get(tone, self.CONSOLIDATED_PROMPTS["executive"])
        user_prompt = self._build_consolidated_prompt(campaign_data)

        # 3. Appeler DeepSeek
        summary_text = await self._call_deepseek(system_prompt, user_prompt)

        # 4. Structurer la rÃ©ponse
        return {
            "executive_summary": summary_text,
            "key_findings": campaign_data.get("key_findings", []),
            "top_recommendations": campaign_data.get("top_actions", []),
            "statistics": campaign_data.get("stats", {}),
            "tone": tone,
            "generated_at": datetime.utcnow().isoformat()
        }

    async def generate_entity_summary(
        self,
        campaign_id: UUID,
        entity_id: UUID,
        tenant_id: UUID,
        tone: str = "executive",
        language: str = "fr"
    ) -> Dict[str, Any]:
        """
        GÃ©nÃ¨re un rÃ©sumÃ© individuel pour une entitÃ© spÃ©cifique.

        Args:
            campaign_id: ID de la campagne
            entity_id: ID de l'entitÃ©
            tenant_id: ID du tenant
            tone: executive, technical, detailed
            language: fr, en

        Returns:
            Dict avec executive_summary, benchmarking, recommendations
        """
        logger.info(f"ğŸ¤– GÃ©nÃ©ration rÃ©sumÃ© entitÃ© {entity_id} - Ton: {tone}")

        # 1. Collecter les donnÃ©es de l'entitÃ©
        entity_data = self._collect_entity_data(campaign_id, entity_id, tenant_id)

        # 2. Construire le prompt
        system_prompt = self.SYSTEM_PROMPT + self.INDIVIDUAL_PROMPTS.get(tone, self.INDIVIDUAL_PROMPTS["executive"])
        user_prompt = self._build_individual_prompt(entity_data)

        # 3. Appeler DeepSeek
        summary_text = await self._call_deepseek(system_prompt, user_prompt)

        # 4. Structurer la rÃ©ponse
        return {
            "executive_summary": summary_text,
            "entity_name": entity_data.get("entity", {}).get("name", "N/A"),
            "benchmarking": entity_data.get("benchmarking", {}),
            "recommendations": entity_data.get("recommendations", []),
            "tone": tone,
            "generated_at": datetime.utcnow().isoformat()
        }

    # ========================================================================
    # MÃ‰THODES PUBLIQUES SYNCHRONES (pour appels depuis contexte non-async)
    # ========================================================================

    def generate_campaign_summary_sync(
        self,
        campaign_id: UUID,
        tenant_id: UUID,
        tone: str = "executive",
        language: str = "fr"
    ) -> Dict[str, Any]:
        """
        Version synchrone de generate_campaign_summary.
        Ã€ utiliser depuis un contexte non-async (ex: job processor).
        """
        logger.info(f"ğŸ¤– [SYNC] GÃ©nÃ©ration rÃ©sumÃ© campagne {campaign_id} - Ton: {tone}")

        # 1. Collecter les donnÃ©es
        campaign_data = self._collect_campaign_data(campaign_id, tenant_id)

        # 2. Construire le prompt
        system_prompt = self.SYSTEM_PROMPT + self.CONSOLIDATED_PROMPTS.get(tone, self.CONSOLIDATED_PROMPTS["executive"])
        user_prompt = self._build_consolidated_prompt(campaign_data)

        # 3. Appeler DeepSeek (version sync)
        summary_text = self._call_deepseek_sync(system_prompt, user_prompt)

        # 4. Structurer la rÃ©ponse
        return {
            "executive_summary": summary_text,
            "key_findings": campaign_data.get("key_findings", []),
            "top_recommendations": campaign_data.get("top_actions", []),
            "statistics": campaign_data.get("stats", {}),
            "tone": tone,
            "generated_at": datetime.utcnow().isoformat()
        }

    def generate_entity_summary_sync(
        self,
        campaign_id: UUID,
        entity_id: UUID,
        tenant_id: UUID,
        tone: str = "executive",
        language: str = "fr"
    ) -> Dict[str, Any]:
        """
        Version synchrone de generate_entity_summary.
        Ã€ utiliser depuis un contexte non-async (ex: job processor).
        """
        logger.info(f"ğŸ¤– [SYNC] GÃ©nÃ©ration rÃ©sumÃ© entitÃ© {entity_id} - Ton: {tone}")

        # 1. Collecter les donnÃ©es de l'entitÃ©
        entity_data = self._collect_entity_data(campaign_id, entity_id, tenant_id)

        # 2. Construire le prompt
        system_prompt = self.SYSTEM_PROMPT + self.INDIVIDUAL_PROMPTS.get(tone, self.INDIVIDUAL_PROMPTS["executive"])
        user_prompt = self._build_individual_prompt(entity_data)

        # 3. Appeler DeepSeek (version sync)
        summary_text = self._call_deepseek_sync(system_prompt, user_prompt)

        # 4. Structurer la rÃ©ponse
        return {
            "executive_summary": summary_text,
            "entity_name": entity_data.get("entity", {}).get("name", "N/A"),
            "benchmarking": entity_data.get("benchmarking", {}),
            "recommendations": entity_data.get("recommendations", []),
            "tone": tone,
            "generated_at": datetime.utcnow().isoformat()
        }

    # ========================================================================
    # COLLECTE DE DONNÃ‰ES
    # ========================================================================

    def _collect_campaign_data(self, campaign_id: UUID, tenant_id: UUID) -> Dict[str, Any]:
        """Collecte les donnÃ©es consolidÃ©es de la campagne."""
        try:
            # Informations campagne
            # Note: la table campaign utilise launch_date/due_date (pas start_date/end_date)
            campaign_query = text("""
                SELECT c.title, c.launch_date, c.due_date, f.name as framework_name
                FROM campaign c
                LEFT JOIN questionnaire q ON c.questionnaire_id = q.id
                LEFT JOIN framework f ON q.framework_id = f.id
                WHERE c.id = CAST(:campaign_id AS uuid)
            """)
            campaign_result = self.db.execute(campaign_query, {"campaign_id": str(campaign_id)}).fetchone()

            # Statistiques globales
            # NOTE: compliance_status utilise les valeurs anglaises: 'compliant', 'non_compliant', 'partial'
            stats_query = text("""
                SELECT
                    COUNT(DISTINCT qr.id) as total_questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status IN ('non_compliant', 'partial') THEN qr.id END) as nc_count
                FROM question_answer qr
                JOIN audit a ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
            """)
            stats_result = self.db.execute(stats_query, {"campaign_id": str(campaign_id)}).fetchone()

            total_questions = stats_result.total_questions or 0
            conformes = stats_result.conformes or 0
            conformity_rate = round((conformes / total_questions * 100), 1) if total_questions > 0 else 0

            # EntitÃ©s
            entities_query = text("""
                SELECT
                    ee.id, ee.name, ee.stakeholder_type,
                    COUNT(DISTINCT qr.id) as questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes
                FROM ecosystem_entity ee
                JOIN audit a ON a.entity_id = ee.id
                JOIN question_answer qr ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                GROUP BY ee.id, ee.name, ee.stakeholder_type
            """)
            entities_results = self.db.execute(entities_query, {"campaign_id": str(campaign_id)}).fetchall()

            entities_summary = []
            for e in entities_results:
                score = round((e.conformes / e.questions * 100), 1) if e.questions > 0 else 0
                entities_summary.append({
                    "name": e.name,
                    "type": e.stakeholder_type or "N/A",
                    "score": score,
                    "level": self._get_maturity_level(score)
                })

            # Domaines
            domains_query = text("""
                SELECT
                    COALESCE(d.code_officiel, d.code) as name,
                    COUNT(DISTINCT qr.id) as questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes
                FROM domain d
                JOIN requirement r ON r.domain_id = d.id
                JOIN question q ON q.requirement_id = r.id
                JOIN question_answer qr ON qr.question_id = q.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                GROUP BY d.id, COALESCE(d.code_officiel, d.code)
                ORDER BY d.code
            """)
            domains_results = self.db.execute(domains_query, {"campaign_id": str(campaign_id)}).fetchall()

            domain_analysis = []
            for d in domains_results:
                rate = round((d.conformes / d.questions * 100), 1) if d.questions > 0 else 0
                domain_analysis.append({
                    "name": d.name,
                    "conformity_rate": rate
                })

            # NC critiques
            nc_query = text("""
                SELECT
                    ee.name as entity_name,
                    COALESCE(d.code_officiel, d.code) as domain_name,
                    q.question_text
                FROM question_answer qr
                JOIN audit a ON qr.audit_id = a.id
                JOIN ecosystem_entity ee ON a.entity_id = ee.id
                JOIN question q ON qr.question_id = q.id
                JOIN requirement r ON q.requirement_id = r.id
                JOIN domain d ON r.domain_id = d.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND qr.compliance_status = 'non_compliant'
                LIMIT 10
            """)
            nc_results = self.db.execute(nc_query, {"campaign_id": str(campaign_id)}).fetchall()

            critical_nc = [
                {"entity_name": nc.entity_name, "domain": nc.domain_name, "control_point": nc.question_text[:80]}
                for nc in nc_results
            ]

            # Statistiques des preuves (attachments)
            attachments_query = text("""
                SELECT
                    COUNT(DISTINCT att.id) as total_attachments,
                    COUNT(DISTINCT CASE WHEN att.virus_scan_status = 'clean' THEN att.id END) as clean_files,
                    COUNT(DISTINCT att.answer_id) as answers_with_evidence,
                    COALESCE(SUM(att.file_size), 0) as total_size_bytes,
                    array_agg(DISTINCT att.attachment_type) FILTER (WHERE att.attachment_type IS NOT NULL) as attachment_types
                FROM answer_attachment att
                JOIN question_answer qr ON att.answer_id = qr.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND att.is_active = true
                  AND att.deleted_at IS NULL
            """)
            attachments_result = self.db.execute(attachments_query, {"campaign_id": str(campaign_id)}).fetchone()

            evidence_stats = {
                "total_attachments": attachments_result.total_attachments or 0,
                "clean_files": attachments_result.clean_files or 0,
                "answers_with_evidence": attachments_result.answers_with_evidence or 0,
                "total_size_mb": round((attachments_result.total_size_bytes or 0) / (1024 * 1024), 2),
                "attachment_types": attachments_result.attachment_types or [],
                "evidence_coverage_rate": round((attachments_result.answers_with_evidence or 0) / total_questions * 100, 1) if total_questions > 0 else 0
            }

            return {
                "campaign": {
                    "title": campaign_result.title if campaign_result else "N/A",
                    "framework_name": campaign_result.framework_name if campaign_result else "N/A"
                },
                "stats": {
                    "total_questions": total_questions,
                    "conformity_rate": conformity_rate,
                    "entities_count": len(entities_summary),
                    "nc_critical": len([e for e in entities_summary if e["score"] < 50]),
                    "nc_major": len([e for e in entities_summary if 50 <= e["score"] < 70])
                },
                "entities_summary": entities_summary,
                "domain_analysis": domain_analysis,
                "critical_nc": critical_nc,
                "evidence_stats": evidence_stats,
                "key_findings": self._extract_key_findings(domain_analysis),
                "top_actions": self._get_top_actions(critical_nc)
            }

        except Exception as e:
            logger.error(f"âŒ Erreur collecte donnÃ©es campagne: {e}")
            # Rollback pour Ã©viter que l'erreur ne bloque les transactions suivantes
            try:
                self.db.rollback()
            except Exception:
                pass
            return {
                "campaign": {"title": "N/A", "framework_name": "N/A"},
                "stats": {"total_questions": 0, "conformity_rate": 0},
                "entities_summary": [],
                "domain_analysis": [],
                "critical_nc": []
            }

    def _collect_entity_data(self, campaign_id: UUID, entity_id: UUID, tenant_id: UUID) -> Dict[str, Any]:
        """Collecte les donnÃ©es d'une entitÃ© spÃ©cifique pour rapport INDIVIDUEL."""
        try:
            logger.info(f"ğŸ” DEBUG _collect_entity_data - campaign_id={campaign_id}, entity_id={entity_id}, tenant_id={tenant_id}")

            # ================================================================
            # 1. INFORMATIONS DE LA CAMPAGNE (contexte essentiel)
            # ================================================================
            campaign_query = text("""
                SELECT
                    c.title as campaign_title,
                    c.description as campaign_description,
                    c.launch_date as start_date,
                    c.due_date as end_date,
                    f.name as framework_name,
                    f.code as framework_code,
                    f.version as framework_version,
                    f.description as framework_description,
                    q.name as questionnaire_name
                FROM campaign c
                LEFT JOIN questionnaire q ON c.questionnaire_id = q.id
                LEFT JOIN framework f ON q.framework_id = f.id
                WHERE c.id = CAST(:campaign_id AS uuid)
            """)
            campaign_result = self.db.execute(campaign_query, {"campaign_id": str(campaign_id)}).fetchone()
            logger.info(f"ğŸ” DEBUG campaign_result: {campaign_result}")

            # ================================================================
            # 2. INFORMATIONS ENRICHIES DE L'ENTITÃ‰
            # ================================================================
            entity_query = text("""
                SELECT
                    ee.name,
                    ee.stakeholder_type,
                    ee.city,
                    ee.country_code,
                    ee.description as entity_description,
                    ee.entity_category as sector,
                    ee.legal_name as employee_count,
                    ee.annual_revenue,
                    cat.name as category_name,
                    cat.entity_category
                FROM ecosystem_entity ee
                LEFT JOIN categories cat ON ee.category_id = cat.id
                WHERE ee.id = CAST(:entity_id AS uuid)
            """)
            entity_result = self.db.execute(entity_query, {"entity_id": str(entity_id)}).fetchone()
            logger.info(f"ğŸ” DEBUG entity_result: {entity_result}")
            logger.info(f"ğŸ” DEBUG entity_name: {entity_result.name if entity_result else 'NONE'}")

            # Score de l'entitÃ©
            # NOTE: compliance_status utilise les valeurs anglaises: 'compliant', 'non_compliant', 'partial'
            score_query = text("""
                SELECT
                    COUNT(DISTINCT qr.id) as total_questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status IN ('non_compliant', 'partial') THEN qr.id END) as nc_count
                FROM question_answer qr
                JOIN audit a ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND a.entity_id = CAST(:entity_id AS uuid)
            """)
            score_result = self.db.execute(score_query, {
                "campaign_id": str(campaign_id),
                "entity_id": str(entity_id)
            }).fetchone()

            total = score_result.total_questions or 0
            conformes = score_result.conformes or 0
            global_score = round((conformes / total * 100), 1) if total > 0 else 0
            logger.info(f"ğŸ” DEBUG score: total={total}, conformes={conformes}, global_score={global_score}%")

            # Benchmarking vs autres entitÃ©s
            benchmark_query = text("""
                SELECT
                    a.entity_id,
                    COUNT(DISTINCT qr.id) as questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes
                FROM question_answer qr
                JOIN audit a ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                GROUP BY a.entity_id
            """)
            benchmark_results = self.db.execute(benchmark_query, {"campaign_id": str(campaign_id)}).fetchall()

            all_scores = []
            for b in benchmark_results:
                score = round((b.conformes / b.questions * 100), 1) if b.questions > 0 else 0
                all_scores.append({"entity_id": str(b.entity_id), "score": score})

            all_scores.sort(key=lambda x: x["score"], reverse=True)
            position = next((i + 1 for i, s in enumerate(all_scores) if s["entity_id"] == str(entity_id)), 0)
            avg_score = round(sum(s["score"] for s in all_scores) / len(all_scores), 1) if all_scores else 0

            # Domaines de l'entitÃ©
            domains_query = text("""
                SELECT
                    COALESCE(d.code_officiel, d.code) as name,
                    COUNT(DISTINCT qr.id) as questions,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status = 'compliant' THEN qr.id END) as conformes,
                    COUNT(DISTINCT CASE WHEN qr.compliance_status IN ('non_compliant', 'partial') THEN qr.id END) as nc_count
                FROM domain d
                JOIN requirement r ON r.domain_id = d.id
                JOIN question q ON q.requirement_id = r.id
                JOIN question_answer qr ON qr.question_id = q.id
                JOIN audit a ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND a.entity_id = CAST(:entity_id AS uuid)
                GROUP BY d.id, COALESCE(d.code_officiel, d.code)
                ORDER BY d.code
            """)
            domains_results = self.db.execute(domains_query, {
                "campaign_id": str(campaign_id),
                "entity_id": str(entity_id)
            }).fetchall()

            domain_analysis = []
            strengths = []
            for d in domains_results:
                rate = round((d.conformes / d.questions * 100), 1) if d.questions > 0 else 0
                domain_analysis.append({
                    "name": d.name,
                    "conformity_rate": rate,
                    "nc": d.nc_count
                })
                if rate >= 80:
                    strengths.append({"title": d.name, "score": rate})

            # ================================================================
            # NON-CONFORMITÃ‰S DÃ‰TAILLÃ‰ES (avec commentaires et recommandations)
            # ================================================================
            # Colonnes vÃ©rifiÃ©es: domain(title, code, code_officiel), requirement(official_code, title),
            # question(question_text), control_point(implementation_guidance)
            nc_query = text("""
                SELECT
                    COALESCE(d.code_officiel, d.code) as domain_name,
                    d.title as domain_full_name,
                    r.official_code as requirement_code,
                    r.title as requirement_title,
                    q.question_text,
                    qr.comment as auditor_comment,
                    qr.compliance_status,
                    cp.implementation_guidance as control_recommendation
                FROM question_answer qr
                JOIN audit a ON qr.audit_id = a.id
                JOIN question q ON qr.question_id = q.id
                LEFT JOIN requirement r ON q.requirement_id = r.id
                LEFT JOIN domain d ON r.domain_id = d.id
                LEFT JOIN control_point cp ON q.control_point_id = cp.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND a.entity_id = CAST(:entity_id AS uuid)
                  AND qr.compliance_status IN ('non_compliant', 'partial')
                ORDER BY d.code, r.official_code
            """)
            nc_results = self.db.execute(nc_query, {
                "campaign_id": str(campaign_id),
                "entity_id": str(entity_id)
            }).fetchall()

            # SÃ©parer NC totales et partielles
            nc_total = [nc for nc in nc_results if nc.compliance_status == 'non_compliant']
            nc_partiel = [nc for nc in nc_results if nc.compliance_status == 'partial']

            non_conformities = {
                "critical": [
                    {
                        "domain": nc.domain_name,
                        "domain_full": nc.domain_full_name,
                        "requirement": nc.requirement_code,
                        "control_point": nc.question_text,
                        "auditor_comment": nc.auditor_comment or "Aucun commentaire",
                        "recommendation": nc.control_recommendation or "Ã€ dÃ©finir"
                    }
                    for nc in nc_total[:10]  # Top 10 NC totales
                ],
                "major": [
                    {
                        "domain": nc.domain_name,
                        "control_point": nc.question_text[:100],
                        "status": "Partiel",
                        "auditor_comment": nc.auditor_comment or ""
                    }
                    for nc in nc_partiel[:10]  # Top 10 NC partielles
                ],
                "total_nc": len(nc_total),
                "total_partial": len(nc_partiel)
            }

            # Statistiques des preuves (attachments) pour cette entitÃ©
            entity_attachments_query = text("""
                SELECT
                    COUNT(DISTINCT att.id) as total_attachments,
                    COUNT(DISTINCT CASE WHEN att.virus_scan_status = 'clean' THEN att.id END) as clean_files,
                    COUNT(DISTINCT att.answer_id) as answers_with_evidence,
                    COALESCE(SUM(att.file_size), 0) as total_size_bytes,
                    array_agg(DISTINCT att.attachment_type) FILTER (WHERE att.attachment_type IS NOT NULL) as attachment_types,
                    array_agg(DISTINCT att.original_filename) FILTER (WHERE att.original_filename IS NOT NULL) as filenames
                FROM answer_attachment att
                JOIN question_answer qr ON att.answer_id = qr.id
                JOIN audit a ON qr.audit_id = a.id
                WHERE qr.campaign_id = CAST(:campaign_id AS uuid)
                  AND a.entity_id = CAST(:entity_id AS uuid)
                  AND att.is_active = true
                  AND att.deleted_at IS NULL
            """)
            entity_attachments_result = self.db.execute(entity_attachments_query, {
                "campaign_id": str(campaign_id),
                "entity_id": str(entity_id)
            }).fetchone()

            evidence_stats = {
                "total_attachments": entity_attachments_result.total_attachments or 0,
                "clean_files": entity_attachments_result.clean_files or 0,
                "answers_with_evidence": entity_attachments_result.answers_with_evidence or 0,
                "total_size_mb": round((entity_attachments_result.total_size_bytes or 0) / (1024 * 1024), 2),
                "attachment_types": entity_attachments_result.attachment_types or [],
                "sample_filenames": (entity_attachments_result.filenames or [])[:10],  # Limiter Ã  10 exemples
                "evidence_coverage_rate": round((entity_attachments_result.answers_with_evidence or 0) / total * 100, 1) if total > 0 else 0
            }

            # Fallback: utiliser le nom du questionnaire si pas de framework
            framework_name = "N/A"
            if campaign_result:
                if campaign_result.framework_name:
                    framework_name = campaign_result.framework_name
                elif campaign_result.questionnaire_name:
                    framework_name = campaign_result.questionnaire_name

            return {
                # âœ… NOUVEAU: Contexte de la campagne
                "campaign": {
                    "title": campaign_result.campaign_title if campaign_result else "N/A",
                    "description": campaign_result.campaign_description if campaign_result else "",
                    "framework_name": framework_name,
                    "framework_code": campaign_result.framework_code if campaign_result and campaign_result.framework_code else "",
                    "framework_version": campaign_result.framework_version if campaign_result and campaign_result.framework_version else "",
                    "framework_description": campaign_result.framework_description if campaign_result and campaign_result.framework_description else "",
                    "questionnaire_name": campaign_result.questionnaire_name if campaign_result and campaign_result.questionnaire_name else "",
                    "start_date": str(campaign_result.start_date) if campaign_result and campaign_result.start_date else "N/A",
                    "end_date": str(campaign_result.end_date) if campaign_result and campaign_result.end_date else "N/A"
                },
                # âœ… ENRICHI: Informations dÃ©taillÃ©es de l'entitÃ©
                "entity": {
                    "name": entity_result.name if entity_result else "N/A",
                    "type": entity_result.stakeholder_type if entity_result else "N/A",
                    "city": entity_result.city if entity_result else "N/A",
                    "country": entity_result.country_code if entity_result else "N/A",
                    "description": getattr(entity_result, 'entity_description', None) or "",
                    "sector": getattr(entity_result, 'sector', None) or "Non spÃ©cifiÃ©",
                    "employee_count": getattr(entity_result, 'employee_count', None) or "Non spÃ©cifiÃ©",
                    "category": getattr(entity_result, 'category_name', None) or "Non catÃ©gorisÃ©",
                    "entity_category": getattr(entity_result, 'entity_category', None) or ""
                },
                "score": {
                    "global_score": global_score,
                    "maturity_level": self._get_maturity_level(global_score),
                    "total_questions": total,
                    "conformes": conformes,
                    "nc_count": score_result.nc_count or 0
                },
                "benchmarking": {
                    "entity_score": global_score,
                    "average_score": avg_score,
                    "position": position,
                    "total_entities": len(all_scores),
                    "performance_vs_average": round(global_score - avg_score, 1)
                },
                "domain_analysis": domain_analysis,
                "non_conformities": non_conformities,
                "evidence_stats": evidence_stats,
                "strengths": strengths,
                "recommendations": self._generate_recommendations(domain_analysis, non_conformities)
            }

        except Exception as e:
            logger.error(f"âŒ Erreur collecte donnÃ©es entitÃ©: {e}")
            # Rollback pour Ã©viter que l'erreur ne bloque les transactions suivantes
            try:
                self.db.rollback()
            except Exception:
                pass
            return {
                "entity": {"name": "N/A"},
                "score": {"global_score": 0},
                "benchmarking": {},
                "domain_analysis": [],
                "non_conformities": {"critical": [], "major": []}
            }

    # ========================================================================
    # CONSTRUCTION DES PROMPTS
    # ========================================================================

    def _build_consolidated_prompt(self, data: Dict[str, Any]) -> str:
        """Construit le prompt utilisateur pour rapport consolidÃ©."""
        entities_text = "\n".join([
            f"{i+1}. {e['name']} ({e['type']})\n   - Score : {e['score']}%\n   - Niveau : {e['level']}"
            for i, e in enumerate(data.get("entities_summary", []))
        ])

        domains_text = "\n".join([
            f"- {d['name']} : {d['conformity_rate']}%"
            for d in data.get("domain_analysis", [])
        ])

        nc_text = "\n".join([
            f"{i+1}. [{nc['entity_name']}] {nc['control_point']}"
            for i, nc in enumerate(data.get("critical_nc", []))
        ])

        # Statistiques des preuves
        evidence = data.get("evidence_stats", {})
        evidence_types = ", ".join(evidence.get("attachment_types", [])) if evidence.get("attachment_types") else "Aucun"

        return f"""
DONNÃ‰ES DE L'AUDIT CONSOLIDÃ‰ :

ğŸ“Š CAMPAGNE
- Titre : {data['campaign']['title']}
- Date : {datetime.now().strftime('%d/%m/%Y')}
- RÃ©fÃ©rentiel : {data['campaign']['framework_name']}

ğŸ“ˆ STATISTIQUES GLOBALES
- Organismes auditÃ©s : {data['stats']['entities_count']}
- Taux de conformitÃ© moyen : {data['stats']['conformity_rate']}%
- NC critiques : {data['stats']['nc_critical']}
- NC majeures : {data['stats']['nc_major']}

ğŸ“ PREUVES DOCUMENTAIRES
- Total piÃ¨ces jointes : {evidence.get('total_attachments', 0)}
- Fichiers vÃ©rifiÃ©s (clean) : {evidence.get('clean_files', 0)}
- Questions avec preuves : {evidence.get('answers_with_evidence', 0)}
- Taux de couverture : {evidence.get('evidence_coverage_rate', 0)}%
- Volume total : {evidence.get('total_size_mb', 0)} MB
- Types de documents : {evidence_types}

ğŸ¢ PERFORMANCE PAR ORGANISME
{entities_text}

ğŸ“Š ANALYSE PAR DOMAINE (moyenne Ã©cosystÃ¨me)
{domains_text}

ğŸ”´ TOP NC CRITIQUES
{nc_text}

---

GÃ‰NÃˆRE LE RÃ‰SUMÃ‰ EXÃ‰CUTIF EN RESPECTANT STRICTEMENT :
- La structure obligatoire du ton demandÃ©
- Maximum 400 mots
- Chiffres exacts tirÃ©s des donnÃ©es ci-dessus
- Mentionner la qualitÃ© et couverture des preuves documentaires

âš ï¸ RÃ‰PONDS UNIQUEMENT AVEC UN JSON VALIDE AU FORMAT:
{
  "summary": "TON RÃ‰SUMÃ‰ COMPLET ICI (avec sections VUE D'ENSEMBLE, POINTS FORTS, RISQUES, etc.)"
}
"""

    def _build_individual_prompt(self, data: Dict[str, Any]) -> str:
        """Construit le prompt utilisateur pour rapport INDIVIDUEL personnalisÃ©."""

        # ================================================================
        # EXTRACTION DES DONNÃ‰ES ENRICHIES
        # ================================================================
        campaign = data.get("campaign", {})
        entity = data.get("entity", {})
        score = data.get("score", {})
        bench = data.get("benchmarking", {})
        evidence = data.get("evidence_stats", {})
        nc = data.get("non_conformities", {})

        # Analyse par domaine
        domains_text = "\n".join([
            f"  â€¢ {d['name']} : {d['conformity_rate']}% de conformitÃ© ({d['nc']} non-conformitÃ©s)"
            for d in data.get("domain_analysis", [])
        ]) or "  Aucune donnÃ©e par domaine"

        # NC critiques avec dÃ©tails enrichis
        nc_critical_list = nc.get("critical", [])
        nc_critical_text = "\n".join([
            f"  {i+1}. [{item.get('domain', 'N/A')}] {item.get('control_point', '')[:150]}\n"
            f"     â†’ Commentaire auditeur : {item.get('auditor_comment', 'Aucun')[:100]}\n"
            f"     â†’ Recommandation : {item.get('recommendation', 'Ã€ dÃ©finir')[:100]}"
            for i, item in enumerate(nc_critical_list[:6])
        ]) or "  Aucune non-conformitÃ© critique"

        # NC partielles
        nc_partial_list = nc.get("major", [])
        nc_partial_text = "\n".join([
            f"  â€¢ [{item.get('domain', 'N/A')}] {item.get('control_point', '')[:100]}"
            for item in nc_partial_list[:5]
        ]) or "  Aucune conformitÃ© partielle"

        # Points forts
        strengths_text = "\n".join([
            f"  â€¢ {s['title']} ({s['score']}% de conformitÃ©)"
            for s in data.get("strengths", [])
        ]) or "  Aucun domaine Ã  plus de 80% de conformitÃ©"

        # Statistiques preuves
        evidence_types = ", ".join(evidence.get("attachment_types", [])) if evidence.get("attachment_types") else "Aucun type spÃ©cifiÃ©"
        sample_files = evidence.get("sample_filenames", [])
        sample_files_text = "\n".join([f"    - {f}" for f in sample_files[:5]]) if sample_files else "    Aucune preuve documentaire fournie"

        # ================================================================
        # CONSTRUCTION DU PROMPT ENRICHI
        # ================================================================
        return f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    RAPPORT D'AUDIT INDIVIDUEL
                    DONNÃ‰ES POUR ANALYSE IA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ CONTEXTE DE LA CAMPAGNE D'AUDIT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Campagne : {campaign.get('title', 'N/A')}
â€¢ Description : {campaign.get('description', 'Non renseignÃ©e')[:200]}
â€¢ RÃ©fÃ©rentiel : {campaign.get('framework_name', 'N/A')} ({campaign.get('framework_code', '')})
â€¢ Version : {campaign.get('framework_version', 'N/A')}
â€¢ PÃ©riode : du {campaign.get('start_date', 'N/A')} au {campaign.get('end_date', 'N/A')}

ğŸ¢ ORGANISME AUDITÃ‰ : {entity.get('name', 'N/A')}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Nom complet : {entity.get('name', 'N/A')}
â€¢ Type d'organisation : {entity.get('type', 'N/A')}
â€¢ CatÃ©gorie : {entity.get('category', 'Non catÃ©gorisÃ©')} ({entity.get('entity_category', '')})
â€¢ Localisation : {entity.get('city', 'N/A')}, {entity.get('country', 'N/A')}
â€¢ Secteur d'activitÃ© : {entity.get('sector', 'Non spÃ©cifiÃ©')}
â€¢ Effectifs : {entity.get('employee_count', 'Non spÃ©cifiÃ©')}
â€¢ Description : {entity.get('description', 'Non renseignÃ©e')[:200]}

âš ï¸ IMPORTANT : Ce rapport concerne UNIQUEMENT l'organisme "{entity.get('name', 'N/A')}".
Toutes les analyses et recommandations doivent Ãªtre spÃ©cifiques Ã  cette entitÃ©.

ğŸ¯ RÃ‰SULTATS DE L'Ã‰VALUATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Score de conformitÃ© global : {score.get('global_score', 0)}%
â€¢ Niveau de maturitÃ© : {score.get('maturity_level', 'N/A')}
â€¢ Points de contrÃ´le Ã©valuÃ©s : {score.get('total_questions', 0)}
â€¢ Conformes : {score.get('conformes', 0)}
â€¢ Non conformes : {nc.get('total_nc', score.get('nc_count', 0))}
â€¢ Partiellement conformes : {nc.get('total_partial', 0)}

ğŸ“ˆ POSITIONNEMENT (Benchmarking)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Score de {entity.get('name', 'N/A')} : {bench.get('entity_score', 0)}%
â€¢ Moyenne des {bench.get('total_entities', 0)} entitÃ©s auditÃ©es : {bench.get('average_score', 0)}%
â€¢ Position : {bench.get('position', 0)}Ã¨me sur {bench.get('total_entities', 0)} entitÃ©s
â€¢ Ã‰cart par rapport Ã  la moyenne : {'+' if bench.get('performance_vs_average', 0) >= 0 else ''}{bench.get('performance_vs_average', 0)}%

ğŸ“Š ANALYSE DÃ‰TAILLÃ‰E PAR DOMAINE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{domains_text}

ğŸ”´ NON-CONFORMITÃ‰S IDENTIFIÃ‰ES ({nc.get('total_nc', 0)} totales)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{nc_critical_text}

âš ï¸ CONFORMITÃ‰S PARTIELLES ({nc.get('total_partial', 0)} contrÃ´les)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{nc_partial_text}

âœ… POINTS FORTS (Domaines >= 80% conformitÃ©)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{strengths_text}

ğŸ“ PREUVES DOCUMENTAIRES FOURNIES PAR {entity.get('name', 'N/A')}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Nombre total de piÃ¨ces jointes : {evidence.get('total_attachments', 0)}
â€¢ Fichiers validÃ©s (sans virus) : {evidence.get('clean_files', 0)}
â€¢ Questions avec preuves : {evidence.get('answers_with_evidence', 0)} / {score.get('total_questions', 0)}
â€¢ Taux de couverture documentaire : {evidence.get('evidence_coverage_rate', 0)}%
â€¢ Volume total : {evidence.get('total_size_mb', 0)} MB
â€¢ Types de documents : {evidence_types}
â€¢ Exemples de fichiers fournis :
{sample_files_text}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    INSTRUCTIONS DE GÃ‰NÃ‰RATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GÃ‰NÃˆRE UN RÃ‰SUMÃ‰ EXÃ‰CUTIF PERSONNALISÃ‰ pour {entity.get('name', 'N/A')} en respectant :

1. PERSONNALISATION OBLIGATOIRE :
   - Mentionner systÃ©matiquement le nom "{entity.get('name', 'N/A')}" dans l'analyse
   - Adapter les recommandations au contexte de l'organisme
   - Faire rÃ©fÃ©rence aux donnÃ©es spÃ©cifiques ci-dessus

2. STRUCTURE Ã€ RESPECTER (selon le ton demandÃ©) :
   - Utiliser les sections obligatoires du ton (executive/technical/detailed)
   - Maximum 500 mots

3. CHIFFRES Ã€ UTILISER :
   - Score : {score.get('global_score', 0)}%
   - Niveau : {score.get('maturity_level', 'N/A')}
   - NC : {nc.get('total_nc', 0)} non-conformitÃ©s
   - Preuves : {evidence.get('evidence_coverage_rate', 0)}% de couverture

4. FOCUS SUR LES Ã‰CARTS :
   - Analyser les domaines les plus faibles
   - Proposer des actions concrÃ¨tes pour {entity.get('name', 'N/A')}

âš ï¸ RÃ‰PONDS UNIQUEMENT AVEC UN JSON VALIDE AU FORMAT:
{{
  "summary": "TON RÃ‰SUMÃ‰ COMPLET ICI PERSONNALISÃ‰ POUR {entity.get('name', 'N/A')}"
}}
"""

    # ========================================================================
    # APPEL IA
    # ========================================================================

    def _extract_content_from_response(self, result: Dict[str, Any]) -> str:
        """
        Extrait le contenu de la rÃ©ponse Ollama/OpenAI-like.
        GÃ¨re plusieurs formats de rÃ©ponse possibles.

        Pour GLM-4.6 en mode "thinking":
        - "thinking" = raisonnement interne du modÃ¨le
        - "content" = rÃ©ponse finale formatÃ©e (ce qu'on veut)

        IMPORTANT: Si content est vide, on utilise thinking comme fallback car
        GLM-4.6 peut parfois mettre le rÃ©sumÃ© complet dans thinking au lieu de content.
        """
        # Format Ollama /api/chat: {"message": {"content": "...", "thinking": "..."}}
        message = result.get("message", {})

        # Log dÃ©taillÃ© pour debug
        msg_keys = list(message.keys()) if isinstance(message, dict) else []
        content_len = len(message.get("content", "")) if isinstance(message, dict) else 0
        thinking_len = len(message.get("thinking", "")) if isinstance(message, dict) else 0
        logger.info(f"ğŸ” Message keys: {msg_keys}, content: {content_len} chars, thinking: {thinking_len} chars")

        content = message.get("content", "")

        # Si content est vide mais thinking existe, utiliser thinking comme fallback
        # GLM-4.6 met parfois tout dans thinking de maniÃ¨re incohÃ©rente
        if not content and "thinking" in message:
            thinking_content = message.get("thinking", "")
            if thinking_content:
                # Marqueurs qui indiquent un rÃ©sumÃ© formatÃ© (pas du raisonnement)
                summary_markers = [
                    "VUE D'ENSEMBLE", "POINTS FORTS", "RISQUES", "RECOMMANDATIONS",
                    "SYNTHÃˆSE", "CONFORMITÃ‰", "NON-CONFORMITÃ‰", "PLAN D'ACTION",
                    "POSITIONNEMENT", "ATOUTS", "AXES D'AMÃ‰LIORATION", "FEUILLE DE ROUTE",
                    "Ã‰TAT DES LIEUX", "CONTRÃ”LES CONFORMES", "Ã‰CARTS TECHNIQUES",
                    "CONTEXTE D'AUDIT", "OBSERVATIONS", "ANALYSE DES Ã‰CARTS"
                ]
                is_formatted_summary = any(marker in thinking_content.upper() for marker in summary_markers)

                # Marqueurs qui indiquent du raisonnement (Ã  Ã©viter)
                reasoning_markers = [
                    "LET ME", "I NEED TO", "I WILL", "FIRST,", "STEP 1",
                    "**DECONSTRUCT", "ANALYZE THE", "UNDERSTAND THE"
                ]
                is_reasoning = any(marker in thinking_content.upper() for marker in reasoning_markers)

                if is_formatted_summary and not is_reasoning:
                    logger.info(f"ğŸ§  RÃ©sumÃ© formatÃ© trouvÃ© dans 'thinking' ({len(thinking_content)} chars)")
                    content = thinking_content
                elif is_reasoning:
                    logger.warning(f"âš ï¸ 'thinking' contient du raisonnement EN ANGLAIS, pas un rÃ©sumÃ© ({len(thinking_content)} chars)")
                    logger.info(f"   Preview thinking: {thinking_content[:300]}...")
                    # Ne pas utiliser le raisonnement, laisser content vide
                else:
                    # Ni rÃ©sumÃ© formatÃ© ni raisonnement clair - utiliser quand mÃªme comme fallback
                    logger.warning(f"âš ï¸ 'thinking' contenu non classifiÃ©, utilisation comme fallback ({len(thinking_content)} chars)")
                    content = thinking_content

        # Fallback: certains modÃ¨les retournent directement {"content": "..."}
        if not content and "content" in result:
            content = result.get("content", "")

        # Fallback: format OpenAI {"choices": [{"message": {"content": "..."}}]}
        if not content and "choices" in result:
            choices = result.get("choices", [])
            if choices and "message" in choices[0]:
                choice_message = choices[0]["message"]
                content = choice_message.get("content", "")
                # VÃ©rifier aussi "thinking" dans le format OpenAI
                if not content and "thinking" in choice_message:
                    content = choice_message.get("thinking", "")

        # Fallback: format {"response": "..."} (certains modÃ¨les Ollama)
        if not content and "response" in result:
            content = result.get("response", "")

        return content

    async def _call_deepseek(self, system_prompt: str, user_prompt: str) -> str:
        """
        Appel Ã  DeepSeek via Ollama (version async).

        Utilise format: "json" pour forcer GLM-4.6 Ã  mettre la rÃ©ponse dans content.
        """
        try:
            # Timeout Ã©levÃ© pour laisser le modÃ¨le travailler
            async with httpx.AsyncClient(timeout=180) as client:
                logger.info(f"ğŸš€ Appel Ollama {self.model} avec format=json...")

                response = await client.post(
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "format": "json",  # âš ï¸ CRUCIAL: Force le contenu dans "content"
                        "stream": False,
                        "keep_alive": "5m",
                        "options": {
                            "temperature": 0.3,
                            "num_predict": 4000,  # AugmentÃ© pour rÃ©sumÃ©s longs
                            "top_p": 0.9,
                            "repeat_penalty": 1.1
                        }
                    }
                )

                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"ğŸ” RÃ©ponse Ollama (clÃ©s): {list(result.keys())}")

                    # Avec format=json, le contenu est TOUJOURS dans message.content
                    content = self._extract_content_from_response(result)

                    # Parser le JSON pour extraire le rÃ©sumÃ©
                    summary = self._parse_json_summary(content)

                    if summary:
                        logger.info(f"âœ… RÃ©sumÃ© IA gÃ©nÃ©rÃ© ({len(summary)} chars)")
                    else:
                        logger.warning(f"âš ï¸ RÃ©sumÃ© vide aprÃ¨s parsing JSON")

                    return summary
                else:
                    logger.error(f"âŒ Erreur Ollama: {response.status_code} - {response.text[:500]}")
                    return self._generate_fallback_summary(user_prompt)

        except Exception as e:
            logger.error(f"âŒ Erreur appel DeepSeek: {e}")
            return self._generate_fallback_summary(user_prompt)

    def _call_deepseek_sync(self, system_prompt: str, user_prompt: str) -> str:
        """
        Appel Ã  DeepSeek via Ollama (version synchrone pour contexte non-async).

        Utilise format: "json" pour forcer GLM-4.6 Ã  mettre la rÃ©ponse dans content.
        """
        try:
            # Timeout Ã©levÃ© pour laisser le modÃ¨le travailler
            with httpx.Client(timeout=180) as client:
                logger.info(f"ğŸš€ [SYNC] Appel Ollama {self.model} avec format=json...")

                response = client.post(
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "format": "json",  # âš ï¸ CRUCIAL: Force le contenu dans "content"
                        "stream": False,
                        "keep_alive": "5m",
                        "options": {
                            "temperature": 0.3,
                            "num_predict": 4000,  # AugmentÃ© pour rÃ©sumÃ©s longs
                            "top_p": 0.9,
                            "repeat_penalty": 1.1
                        }
                    }
                )

                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"ğŸ” RÃ©ponse Ollama sync (clÃ©s): {list(result.keys())}")

                    # Log dÃ©taillÃ© de la structure message
                    message = result.get("message", {})
                    logger.info(f"ğŸ” Message keys: {list(message.keys()) if isinstance(message, dict) else 'N/A'}")
                    content_len = len(message.get('content', '')) if isinstance(message, dict) else 0
                    logger.info(f"ğŸ” content length: {content_len}")

                    # Afficher les 500 premiers caractÃ¨res de content
                    if message.get("content"):
                        logger.info(f"ğŸ“ CONTENT (500 chars): {message.get('content', '')[:500]}")

                    # Avec format=json, le contenu est TOUJOURS dans message.content
                    content = self._extract_content_from_response(result)

                    # Parser le JSON pour extraire le rÃ©sumÃ©
                    summary = self._parse_json_summary(content)

                    if summary:
                        logger.info(f"âœ… RÃ©sumÃ© IA gÃ©nÃ©rÃ© (sync) ({len(summary)} chars)")
                        logger.info(f"ğŸ“„ RÃ‰SUMÃ‰ FINAL (500 chars): {summary[:500]}")
                    else:
                        logger.warning(f"âš ï¸ RÃ©sumÃ© vide aprÃ¨s parsing JSON (sync)")

                    return summary
                else:
                    logger.error(f"âŒ Erreur Ollama: {response.status_code} - {response.text[:500]}")
                    return self._generate_fallback_summary(user_prompt)

        except Exception as e:
            logger.error(f"âŒ Erreur appel DeepSeek (sync): {e}")
            return self._generate_fallback_summary(user_prompt)

    def _parse_json_summary(self, content: str) -> str:
        """
        Parse le JSON retournÃ© par l'IA et extrait le rÃ©sumÃ©.

        Le modÃ¨le doit retourner: {"summary": "..."}

        GÃ¨re plusieurs cas:
        - JSON valide avec clÃ© "summary"
        - JSON avec autres clÃ©s (fallback sur premiÃ¨re valeur string longue)
        - Texte brut (fallback si pas JSON valide)
        """
        import json

        if not content:
            logger.warning("âš ï¸ Contenu vide reÃ§u pour parsing JSON")
            return ""

        # Nettoyer le contenu (enlever backticks markdown si prÃ©sents)
        cleaned = content.strip()
        if cleaned.startswith("```"):
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            else:
                cleaned = cleaned[3:]
            if "```" in cleaned:
                cleaned = cleaned.split("```")[0]
            cleaned = cleaned.strip()

        try:
            # Tenter de parser le JSON
            data = json.loads(cleaned)

            # Cas 1: ClÃ© "summary" directe
            if isinstance(data, dict) and "summary" in data:
                summary = data["summary"]
                logger.info(f"âœ… JSON parsÃ© avec succÃ¨s - clÃ© 'summary' trouvÃ©e ({len(summary)} chars)")
                return summary

            # Cas 2: Chercher une clÃ© contenant "summary" ou "rÃ©sumÃ©"
            if isinstance(data, dict):
                for key in data:
                    if "summary" in key.lower() or "rÃ©sumÃ©" in key.lower() or "resume" in key.lower():
                        summary = data[key]
                        if isinstance(summary, str) and len(summary) > 50:
                            logger.info(f"âœ… JSON parsÃ© - clÃ© '{key}' utilisÃ©e ({len(summary)} chars)")
                            return summary

                # Cas 3: Fallback sur premiÃ¨re valeur string longue
                for key, value in data.items():
                    if isinstance(value, str) and len(value) > 100:
                        logger.warning(f"âš ï¸ Fallback: utilisation de la clÃ© '{key}' ({len(value)} chars)")
                        return value

            # Cas 4: Si c'est une string directe
            if isinstance(data, str) and len(data) > 50:
                logger.info(f"âœ… JSON Ã©tait une string directe ({len(data)} chars)")
                return data

            logger.warning(f"âš ï¸ Structure JSON inattendue: {str(data)[:200]}")
            return str(data) if data else ""

        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON invalide, utilisation du contenu brut: {e}")
            # Si le contenu ressemble Ã  un rÃ©sumÃ© (pas du raisonnement en anglais)
            summary_markers = ["VUE D'ENSEMBLE", "POINTS FORTS", "SYNTHÃˆSE", "POSITIONNEMENT"]
            reasoning_markers = ["LET ME", "I NEED TO", "STEP 1", "FIRST,"]

            is_summary = any(marker in cleaned.upper() for marker in summary_markers)
            is_reasoning = any(marker in cleaned.upper() for marker in reasoning_markers)

            if is_summary and not is_reasoning:
                logger.info(f"ğŸ“ Contenu brut utilisÃ© comme rÃ©sumÃ© ({len(cleaned)} chars)")
                return cleaned
            elif is_reasoning:
                logger.warning(f"âš ï¸ Contenu rejetÃ© (raisonnement en anglais)")
                return ""
            else:
                # Utiliser quand mÃªme si assez long
                if len(cleaned) > 200:
                    logger.info(f"ğŸ“ Contenu brut utilisÃ© (fallback) ({len(cleaned)} chars)")
                    return cleaned
                return ""

    def _generate_fallback_summary(self, user_prompt: str) -> str:
        """GÃ©nÃ¨re un rÃ©sumÃ© basique en cas d'erreur IA."""
        return """VUE D'ENSEMBLE
Le rÃ©sumÃ© IA n'a pas pu Ãªtre gÃ©nÃ©rÃ© automatiquement. Veuillez consulter les donnÃ©es dÃ©taillÃ©es du rapport pour une analyse complÃ¨te.

âš ï¸ NOTE
Ce rÃ©sumÃ© a Ã©tÃ© gÃ©nÃ©rÃ© en mode dÃ©gradÃ©. Pour un rÃ©sumÃ© complet, vÃ©rifiez la connexion au service IA (Ollama/DeepSeek).
"""

    # ========================================================================
    # UTILITAIRES
    # ========================================================================

    def _get_maturity_level(self, score: float) -> str:
        """DÃ©termine le niveau de maturitÃ© selon le score."""
        if score >= 90:
            return "OptimisÃ©"
        elif score >= 75:
            return "GÃ©rÃ©"
        elif score >= 60:
            return "DÃ©fini"
        elif score >= 40:
            return "RÃ©pÃ©table"
        else:
            return "Initial"

    def _extract_key_findings(self, domain_analysis: List[Dict]) -> List[str]:
        """Extrait les constats clÃ©s des domaines."""
        findings = []
        for d in domain_analysis:
            if d["conformity_rate"] >= 80:
                findings.append(f"Point fort : {d['name']} ({d['conformity_rate']}%)")
            elif d["conformity_rate"] < 50:
                findings.append(f"Point d'attention : {d['name']} ({d['conformity_rate']}%)")
        return findings[:5]

    def _get_top_actions(self, critical_nc: List[Dict]) -> List[str]:
        """GÃ©nÃ¨re les actions prioritaires."""
        actions = []
        domains_seen = set()
        for nc in critical_nc:
            domain = nc.get("domain", "")
            if domain and domain not in domains_seen:
                actions.append(f"RemÃ©diation {domain}")
                domains_seen.add(domain)
            if len(actions) >= 3:
                break
        return actions

    def _generate_recommendations(self, domain_analysis: List[Dict], non_conformities: Dict) -> List[Dict]:
        """GÃ©nÃ¨re des recommandations basÃ©es sur l'analyse."""
        recommendations = []

        # Recommandations par domaine faible
        for d in sorted(domain_analysis, key=lambda x: x["conformity_rate"]):
            if d["conformity_rate"] < 60:
                recommendations.append({
                    "domain": d["name"],
                    "priority": "high" if d["conformity_rate"] < 40 else "medium",
                    "action": f"AmÃ©liorer la conformitÃ© du domaine {d['name']}"
                })
            if len(recommendations) >= 5:
                break

        return recommendations
