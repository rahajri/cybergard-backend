"""
Service de cache pour les r√©sultats IA
Wrapper autour des services DeepSeek/Ollama avec mise en cache Redis
"""

import hashlib
import json
import logging
from typing import Any, Optional, Dict

from src.utils.redis_manager import redis_manager, cache_result

logger = logging.getLogger(__name__)


class CachedAIService:
    """
    Service de cache pour les r√©sultats d'IA
    Encapsule les appels aux mod√®les IA avec mise en cache Redis
    """

    @staticmethod
    def generate_prompt_hash(
        prompt: str,
        model: str,
        temperature: float = None,
        max_tokens: int = None,
        **kwargs
    ) -> str:
        """
        G√©n√®re un hash unique pour un prompt et ses param√®tres

        Args:
            prompt: Le prompt texte
            model: Nom du mod√®le
            temperature: Temp√©rature
            max_tokens: Tokens max
            **kwargs: Autres param√®tres

        Returns:
            Hash SHA256 tronqu√©
        """
        # Cr√©e une cl√© unique bas√©e sur tous les param√®tres
        params = {
            "prompt": prompt,
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }

        # Trie les cl√©s pour garantir la reproductibilit√©
        canonical = json.dumps(params, sort_keys=True, ensure_ascii=False)

        # G√©n√®re le hash
        return hashlib.sha256(canonical.encode()).hexdigest()[:16]

    @staticmethod
    async def get_or_generate(
        prompt: str,
        model: str,
        generator_func,
        ttl: int = 3600,
        temperature: float = None,
        max_tokens: int = None,
        force_refresh: bool = False,
        **kwargs
    ) -> Any:
        """
        R√©cup√®re le r√©sultat du cache ou g√©n√®re avec l'IA

        Args:
            prompt: Le prompt
            model: Nom du mod√®le
            generator_func: Fonction de g√©n√©ration async
            ttl: Dur√©e de vie du cache en secondes
            temperature: Temp√©rature
            max_tokens: Tokens max
            force_refresh: Force la r√©g√©n√©ration
            **kwargs: Param√®tres additionnels

        Returns:
            R√©sultat de l'IA (du cache ou nouvellement g√©n√©r√©)
        """
        # G√©n√®re le hash du prompt
        prompt_hash = CachedAIService.generate_prompt_hash(
            prompt=prompt,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )

        # V√©rifie le cache si pas de force refresh
        if not force_refresh and redis_manager.is_connected:
            cached = redis_manager.get_cached_ai_result(model, prompt_hash)
            if cached is not None:
                logger.info(f"‚úÖ Cache HIT pour {model}:{prompt_hash[:8]}")
                return cached

        logger.info(f"‚ö†Ô∏è Cache MISS pour {model}:{prompt_hash[:8]} - G√©n√©ration...")

        # G√©n√®re le r√©sultat
        result = await generator_func(prompt, **kwargs)

        # Met en cache si Redis disponible
        if redis_manager.is_connected:
            success = redis_manager.cache_ai_result(
                model=model,
                prompt_hash=prompt_hash,
                result=result,
                ttl=ttl
            )
            if success:
                logger.info(f"üíæ R√©sultat mis en cache: {model}:{prompt_hash[:8]}")
            else:
                logger.warning(f"‚ö†Ô∏è √âchec mise en cache: {model}:{prompt_hash[:8]}")

        return result

    @staticmethod
    async def generate_control_points_cached(
        requirement_text: str,
        generator_service,
        model: str = "deepseek",
        ttl: int = 7200,  # 2 heures pour les points de contr√¥le
        **kwargs
    ) -> list:
        """
        G√©n√®re des points de contr√¥le avec cache

        Args:
            requirement_text: Texte de l'exigence
            generator_service: Service de g√©n√©ration
            model: Mod√®le utilis√©
            ttl: TTL du cache
            **kwargs: Param√®tres additionnels

        Returns:
            Liste des points de contr√¥le g√©n√©r√©s
        """
        async def generator(prompt, **gen_kwargs):
            # Appelle la m√©thode du service
            return await generator_service.generate_control_points(
                requirement_text=prompt,
                **gen_kwargs
            )

        return await CachedAIService.get_or_generate(
            prompt=requirement_text,
            model=model,
            generator_func=generator,
            ttl=ttl,
            **kwargs
        )

    @staticmethod
    async def generate_questions_cached(
        control_point_text: str,
        generator_service,
        model: str = "deepseek",
        ttl: int = 7200,  # 2 heures pour les questions
        **kwargs
    ) -> list:
        """
        G√©n√®re des questions avec cache

        Args:
            control_point_text: Texte du point de contr√¥le
            generator_service: Service de g√©n√©ration
            model: Mod√®le utilis√©
            ttl: TTL du cache
            **kwargs: Param√®tres additionnels

        Returns:
            Liste des questions g√©n√©r√©es
        """
        async def generator(prompt, **gen_kwargs):
            return await generator_service.generate_questions(
                control_point_text=prompt,
                **gen_kwargs
            )

        return await CachedAIService.get_or_generate(
            prompt=control_point_text,
            model=model,
            generator_func=generator,
            ttl=ttl,
            **kwargs
        )

    @staticmethod
    def invalidate_cache(
        model: Optional[str] = None,
        prompt_hash: Optional[str] = None
    ) -> int:
        """
        Invalide le cache IA

        Args:
            model: Mod√®le sp√©cifique (None = tous)
            prompt_hash: Hash sp√©cifique (None = tous pour le mod√®le)

        Returns:
            Nombre de cl√©s supprim√©es
        """
        if not redis_manager.is_connected:
            logger.warning("Redis non connect√© - impossible d'invalider le cache")
            return 0

        if prompt_hash and model:
            # Supprime une entr√©e sp√©cifique
            key = f"ai:{model}:{prompt_hash}"
            success = redis_manager.delete(key)
            return 1 if success else 0
        elif model:
            # Supprime tout le cache d'un mod√®le
            return redis_manager.clear_ai_cache(model)
        else:
            # Supprime tout le cache IA
            return redis_manager.clear_ai_cache()

    @staticmethod
    def get_cache_stats(model: Optional[str] = None) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques du cache IA

        Args:
            model: Mod√®le sp√©cifique

        Returns:
            Statistiques du cache
        """
        if not redis_manager.is_connected:
            return {
                "status": "disconnected",
                "message": "Redis non disponible"
            }

        try:
            pattern = f"ai:{model}:*" if model else "ai:*"
            client = redis_manager.client

            if not client:
                return {"status": "error", "message": "Client Redis non disponible"}

            keys = client.keys(pattern)

            return {
                "status": "ok",
                "model": model or "all",
                "total_cached_results": len(keys),
                "pattern": pattern
            }
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration stats cache: {e}")
            return {
                "status": "error",
                "message": str(e)
            }


# Instance globale
cached_ai_service = CachedAIService()
